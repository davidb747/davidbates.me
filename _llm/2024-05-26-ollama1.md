---
layout: article
title: "Ollama로 Llama3 실행하기 (1/2): 로컬 서버 띄우기"
aside:
  toc: true
cover: /assets/ml/llama3_thumbnail_1.jpg
excerpt: Ollama를 이용한 LLM 모델 로컬 서빙 방법에 대해 알아보겠습니다.
---



몇주전 메타에서 발표한 오픈소스인 Llama3가 핫하죠?

저도 몇번 써봤는데, ChatGPT 3.5보다는 확실히 성능이 개선된걸 많이 느꼈습니다. 

소신발언하자면 아직까지 GPT-4를 뛰어넘었는지는 잘 모르겠더라고요 ㅎㅎ...

<br>

그래도 Llama3는 무료라는 어마어마한 강점이 있죠. 

그리고 컴퓨팅 환경만 갖춰져 있다면 로컬에 서버를 두고 얼마든지 사용할 수 있습니다. 

저도 회사 서버에 Llama3 서버를 띄워두고 사용하고 있거든요.

<br>

그래서 오늘은 Llama3 로컬서버를 구축하는 방법에 대해 공유드릴까해요. 

대부분의 내용은 유튜버 '테디노트'님의 [이 영상](https://www.youtube.com/watch?v=12CuUQIPdM4&feature=youtu.be)에서 참고하였으니 직접 가서 보시는 것도 권장드립니다.

<br>

'Ollama로 llama3 실행하기'는 2개의 포스팅으로 나눠서 업로드할 예정입니다. 

첫번째 포스팅인 본 포스팅은 가장 기본이 되는 내용으로, Ollama에서 제공하는 기본 Llama3 모델을 실행하는 법을 다룰 거에요. 

두번째 포스팅에서는 한국어로 Fine tuning된 custom Llama3 모델을 실행하는 방법에 대해 다룰 예정입니다!

<br>

<br>

# 1. 준비물 


Ollama : LLM 로컬 서버 구축용.
{:.info}

로컬 서버 구축에는 Ollama라는 오픈소스를 활용할거에요. 

Ollama는 Llama와 같은 LLM 모델 실행 환경을 제공하는 오픈소스에요.

<br>

<br>

# 2. Ollama로 Llama3 로컬 서버 띄우기 


## 2-1. Ollama 다운로드 

먼저 Ollama를 다운로드 해주셔야하는데요. 

[여기](https://ollama.com/download/linux)서 다운로드하시면 돼요. 

각자 자신의 OS에 맞게 다운로드해주세요. 

<br>

<br>

## 2-2. Llama3 다운 받기 

![ollama_model](/assets/ml/ollama_model.png){:.rounded}

Ollama의 모델 페이지에 보면, Ollama에서 기본적으로 제공하는 모델들을 볼 수 있어요. 

위의 이미지를 보시면, 가장 위에 llama3가 있는거 보이시죠? 

<br>

오늘은 저 모델을 로컬에 다운 받아 실행할거에요. 

터미널에서 아래의 코드를 실행하면 모델을 다운로드 받을 수 있어요.

```shell
ollama pull llama3
```

<br>

이제 잘 받아졌는지 확인해볼까요? 

```shell
ollama list
```

<br>

![ollama_run](/assets/ml/ollama_terminal.png)

ollama list를 실행하면 내가 다운받은 모델들이 출력돼요. 

방금 다운로드 받은 llama3 모델이 출력된다면 무사히 다운로드 받은거에요.

참고로 해당 모델은 `llama3 8B` 모델입니다. 

<br>

<br>

## 2-3. Llama3 로컬 서버 실행하기 


이제 실행하면 됩니다. 

```shell
# llama3 8B 모델 실행하기 
ollama run llama3

# llama3 70B 모델 실행하기
# 추가로 모델 다운로드가 필요할 수도 있음
ollama run llama3:70b
```

![ollama_run_result](/assets/ml/ollama_terminal_2.png)

위의 코드를 실행하면 2-2에서 다운로드 받은 llama3를 실행할 수 있어요. 

그리고 저렇게 바로 ollama를 사용할 수 있는 채팅 인터페이스가 출력돼요. 

저기에 프롬프트를 입력하면은 llama3를 사용할 수 있습니다. 

<br>

<br>

# 3. 마치며 

Ollama로 Llama3 로컬 서버를 띄우는 방법에 대해 설명 드렸는데요. 

생각보다 어렵진 않으셨을거에요. 

<br>

이제 마음껏 사용하시면 되는데요. 

아쉽게도 Llama3가 한국어를 공식 지원하지는 않습니다. 

그래서 한국인들이 Fine Tuning한 Llama3를 불러와서 사용해야하는데요. 

<br>

다음 포스팅에서는 한국어 Fine tuning된 Llama3를 실행하는 방법에 대해 공유해드릴게요. 

<br>

혹시 진행하시다가 막히거나 모르는 부분이 있으시면 언제든지 편하게 댓글 남겨주세요.

<br>

<br>