---
layout: article
title: Markov Decision Processes (MDP)
date: 2022-04-30
sidebar:
  nav: "docs-en"
---

<!DOCTYPE HTML>
<html>


<!-- Adding MathJAX -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
  <script async="true" src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=AM_CHTML"> </script>
<!-- End of MathJAX -->


<body>
			<p>Learner and Decision-maker &rarr; <i>agent</i><br>
			Things it interacts with, outside agent &rarr; <i>environment</i></p>
			<p>In a finite Markov Decision Process (MDP):
			<ol>
				<li>The agent and the environment interact at discrete time steps, represented as `t=0,1,2,3,..`</li>
				<li>At each time step, `t`, the agent recieves some information about the environment's state, denoted as `S_(t)&isin;S`. Based on this state, the agent selects an action, `A_(t)&isin;A(S)`</li>
				<li>One time step later, the agent recieves a numerical reward, `R_(t+1) &isin;&Rscr; &sub;&Ropf;`, and find itself in a new state, `S_(t+1)`</li>
				<li>The interaction sequence follows the pattern: `S_(0),A_(0),R_(1),S_(1),A_(1),R_(2),S_(2),A_(2),R_(3),....`</li>
			</ol>
			</p>
			<p>
				In a finite MDP, the probability distribution of the random variable, `R_(t)` and `S_(t)` depend only on the preceding state & action i.e., if `s'&isin;S` and `r&isin;&Rscr;`, then<br>
				&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`P(s',r | s,a)&esdot; Pr{S_(t)=s',R_(t)=r | S_(t-1)=s,A_(t-1)=a}`
			</p>
			<p>The function <i>p</i> defines the dynamics of the MDP. It is a deterministic function of four agruments,`p:S&times;R&times;S&times;A &rarr;[0,1]` and specifies a probability distribution for each choice of state `s` and action `a` satisfying:<br>
				&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`\sum_{s'&isin;S}\sum_{r&isin;R} p(s',r | s,a)=1` for all `s&isin;S, a&isin;A_(s)`
			</p>
			<p><b>Markov Decision Processes (MDP):</b></p> 
			<p>MDP, therefore, is a decision process in which the probability of each value for `S_(t)` and `R_(t)` depends only on the preceding state and action `S_(t-1)` and `A_(t-1)` and not on earlier state/action.<br>
			In essence, if the state includes information about all aspects of the past agent-environment interaction, then the state is said to have a <i>Markov Property</i>.</p>
			
			<p>Using the dynamic function, p, we can compute expected reward for state-action pair, denoted as `r:S&times;A&rarr;R`</p>
			<p>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`r(s,a) &esdot;`&Eopf;`&#91;R_(t) | S_(t-1)=s, A_(t-1)=a&#93;=\sum_{r&isin;R}r\sum_{s'&isin;S}p(s',r | s,a)`</p>
			<p>Similarly, we can compute the expected reward for state-action-next state denoted as `r:S&times;A&times;S&rarr;R`:</p>
			<p>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`r(s,a,s')&esdot;`&Eopf;`&#91;R_(t) | S_(t-1)=s,A_(t-1)=a,S_(t)=s'&#93;=\sum_{r&isin;R}r\frac{p(s',r | s,a)}{p(s' |s,a)}`</p>

			<p><b>Goals and Rewards :</b></p>
			<p>In Reinforcement Learning, goal of the agent is to maximise the reward, `R` which can be stated as <u>reward hypothesis</u> below:</p>
			<p><i>That all of what we mean by goals and purposes can be well thought of as the maximization of the exptected value of the cumulative sum of a received scalar signal (called reward).</i></p>
			<p>In other words, if the exptected return at time step `t` is denoted by ` G_(t)` and the final time step is `T`, then</p>
			<p>&emsp;`G_(t)=R_(t+1) + R_(t+2) + R_(t+3) + R_(t+4) + ........ + R_(T)` </p>
			<p>The objective is to maximise the cumulative reward received at each time step, `R_(t+1), R_(t+2),..`, and so on, ultimately maximising `G_(t)`</p>
			<p>MDP tasks of maximising `G_(t)` can be classified into two types: <u>episodic taks</u> and <u>continuing task</u>.</p>
			<p>In episodic task, the agent-environment breaks into subsection called <i>episodes</i>. Each episode ends in a special state called a terminal state, and independent of how the previous episode ended.
			On the other hand, in continuing tasks, the agent-environment oesn't break into identifiable episodes and goes on continually without limit</p>

			<p><b>Discounting :</b></p>
			<p>In some approaches, the agent selects actions, `A_(t)` to maximize the sum of discounted return, `G_(t)` over the future. It is defined as:</p>
			<p>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`G_(t)&esdot; R_(t+1) + &gamma; R_(t+2) +&gamma;^2R_(t+3) + ....  = \sum_{k=0}^&infin;&gamma;^k R_(t+k+1)` &emsp;&emsp;&emsp;..(1)</p>
			<p>Here, `&gamma;` is <i>Discount Rate</i> and `0&le;&gamma;&le;1`. Importantly, if `&gamma;=0`, the agent is concerned only with the maximizing immediate reward as the future terms `&gamma; R_(t+2) +&gamma;^2R_(t+3) + ....` become `0`.
			On the other hand, if `&gamma;&sime;1`, the agent's return objective takes future reward into account more strongly.</p>

			<p>Futhermore, if we look more closely into eq(1), we can observe that returns on successive time steps are related to each other as: </p>
				&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`G_(t)&esdot; R_(t+1) + &gamma; R_(t+2) +&gamma;^2R_(t+3) + &gamma;^3R_(t+4) + ....`<br>
				&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`= R_(t+1) + &gamma;(R_(t+2) +&gamma;R_(t+3) + &gamma;^2R_(t+4) + ....)`<br>
				&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`= R_(t+1) + &gamma;G_(t+1)`<br>
			<p>Note that it works for all time steps `t&lt;T` , even if termination occurs at `t+1`. Also, if `G_(t)` is expected of consisting infinite terms where reward is non-zero and constant `&gamma;&lt;1`, we can write `G_(t)` as:</p>
				&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`G_(t)=\sum_{k=0}^&infin; &gamma;^k = \frac{1}{1-&gamma;}`
</body>
</html>
