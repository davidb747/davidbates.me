---
layout: article
title: Encoder-Decoder and Transformers
sidebar:
  nav: "docs-en"
---

<!DOCTYPE HTML>
<html>

<!--- Adding Google Analytics -->
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-154990580-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-154990580-2');
</script>
<!-- End of Google Analytics Code -->
<!-- Adding MathJAX -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
  <script async="true" src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=AM_CHTML"> </script>
<!-- End of MathJAX -->


<body>
	
        <p>Date: 14<sup>th</sup> July 2022</p>
        <h2>Encoder-Decoder</h2>
        <p>Encoder-Decoder architecture is based on Recurrent Neural Network(RNN). In RNN, we have a hidden state `h` which takes a variable length sequence `X = (x_(1),x_(2),...x_(T))`  to generate output `y`. At any time step `t`, hidden state `h_(t)` is updated by :</p>
        <p style="font-size: 18px;">&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`h_(t) = f(h_(t-1),x_(t))`&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;..<i>eq(1)</i></p>
        <p>where `f` is a non-linear activation function. It can be element-wise sigmoid function or LSTM. Output at eah time step is the conditional distribution `p(x_(t) | x_(t-1),....x_(1))`</p>
        <p>In Encoder-Decoder, we have a RNN encoder that <i>encodes</i> a variable-length sequence into a fixed-length vector, and a RNN decoder that <i>decodes</i> fixed-length vector back to variable-length sequence. 
        Mathematically, it is a method to learn the conditional distribution over a variable-length sequence conditioned on yet another variable-length sequence e.g, `P(y_(1),y_(2),....,y_(T') | x_(1),x_(2),....,x_(T))`, where importantly, input sequence `T` and output sequence `T'` could vary.
        After reading each input at all time steps, hidden state of encoder RNN is a summary `C` of whole input sequence as shown in image below :</p>
        <p><img src="assets/img/encdec.png" width="350" height="250" alt="encoder-decoder" class="image_full"><br>
        &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;RNN Encoder-Decoder Architecture</p>
        <p>Next decoder generate the output sequence by predicting the next symbol `y_(t)`. Here, in RNN decoder both `y_(t)` and `h_(t)` is conditioined on `y_(t-1)` and summary (or context vector) `C`. Thus, hidden state of decoder can be computed by :</p>
        <p class="formula">&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`h_(t) = f(h_(t-1), y_(t-1),C)`&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;...<i>eq(2)</i></p>
        <p>Decoder defines a probability over the translation `y` by decomposing the joint probability into the ordered conditional :</p>
        <p class="formula">&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`P(y) = &Pi;_{t=1}^T p(y_(t) | {y_(1),....,y_(t-1)},c)`</p>
        <p>and the conditional distribution of the next symbol is (if `g` represents activation such as soft-max) :</p>
        <p class="formula">&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`P(y_(t) | y_(t-1),y_(t-2),.....,y_(1),C) = g(h_(t),y_(t-1),C)`&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;....<i>eq(3)</i></p>
        <p>This model of RNN Encoder-Decoder then jointly trained to maximize the conditional log-likelyhood</p>
        <p class="formula">&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`max_{&theta;}\frac{1}{N}\sum_{n=1}^Nlog&nbsp;p_(&theta;)(y_(n) | X_(n))`</p>
        <p>where &theta; is the set of the model parameters and each `(x_(n),y_(n))` is an <i>(input sequence, output sequence)</i> pair from training set.</p>
        <p> (Cho et al., 2014b) prposed to use <a href="lstm.html#ls">LSTM</a> <i>(paper says it is <a href="lstm.html#ls">LSTM</a> but it is <a href="lstm.html#ls">GRU</a>, for having reset and update gates)</i> in hidden state. When reset gate is close to `0`, hidden state is forced to ignore previous hidden state and continue with current input. This effectively allows the hidden state to drop any irrelevant information. Update gate, on the other hand, controls how much information from the previous hidden state get to pass on to next hidden state. Combination of these two gates, thus, allows more compact and improved representation of output sequence.</p>
        <p>(Sutskever et al., 2014), however, proposed three changes : <br>
        1. Two LSTM to be used - one for encoder, another for decoder.<br>
        2. Deep LSTM should be used. LSTM with four layers is implemented.<br>
        3. To reverse the order of input sequence. For instance, instead of mapping the input sequence `a,b,c` to `&alpha;,&beta;,&gamma;` it asked to map `c,b,a` to `&alpha;,&beta;,&gamma;` </p>
        
        <h2>Bahdanau Model</h2>
        <p>Bahdanau model is based on RNN Encoder-Decoder that learns to align and translate simultaneously. This model also uses BiDirectional RNN (BiRNN) for Encoder.</p>
        <p>BiRNN Encoder consists of forward and backward RNN. The forward RNN `\vecf` reads the input sequence in order (from `x_(1)` to `x_(T)`) and provides forward hidden states i.e, `\vech_(1),....,\vech_(T)`.<br>
        The backward RNN `f` reads sequence in reverse (from `x_(T),..,x_(1))` providing sequence of backward hidden states `h_(1)^&larr;,....,h_(T)^&larr;`<br>
        This way we obtain the annotation for each word `x_(j)` by concatenating the forward hidden and backwards hidden states i.e, `h_(j) = [\vech_(j);h_(j)^&larr;]^T`(Bahdanau et al., 2014)</p>
        <p>Each annotations `h_(i)` obtained this way contains information about the whole input sequence with a strong focus on the parts surrounding the `i-th` word of the input sequence.</p>
        <p>The context vector `c_(i)` is then computed as a weighted sum of these annotations `h_(i)` as :</p>
        <p class="formula">&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`c_(i) = \sum_{j=1}^(T_(x)) &alpha;_(ij)h_(j)`</p>
        <p>and, the weight of each annotation `h_(j)` is computed as :</p>
        <p class="formula">&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`&alpha;_(ij) = \frac{exp(e_(ij))}{\sum_{k=1}^(T_(x))(exp(e_(ik)))}`</p>
        <p>where, `e_(ij) = a(s_(i-1),h_(j))` is an <i>alignment mode</i> which scores how well the inputs around position `j` and the ouput at position `i` match. Now, the Decoder part - Bahdanau model Decoder, in comparison with eq(3) define each conditional probability as :</p>
        <p class="formula">&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`p(y_(i) | y_(1),...,y_(i-1),X) = g(y_(i-1),s_(i),c_(i))`</p>
        <p>where `s_(i)` is an RNN hidden state for time `i` computed by :</p>
        <p class="formula">&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`s_(i) = f(s_(i-1),y_(i-1),c_(i))`</p>
        <p>This way Bahdanau model achieve significantly improved translation in comparison with conventional Encoder-Decoder, and especially in the cases of longer sentences. </p>
        

        <h2>Attention Mechanism and Transformers</h2>
        <p>Transformers architecture is based on Encoder-Decoder, but uses Attention Mechanism. Lets first discuss Attention Mechanism and key things in it.<br>
        Like visual world, Attention Mechanism is two component framework, here :<br>
        volitional cues &rarr; queries<br>
        nonvolitional cues &rarr; keys<br>
        Interaction between queries and keys results <i>Attention Pooling</i>, and based on kind of it, we aggregate values(sensory inputs) to generate output. </p>
        <p><b>Non-parametric Attention Pooling</b><br>
        Given a input-output pair `{(x_(1),y_(1)),...,(x_(n),y_(n))}` if we have to find `f(x)` for new input `x_(n+1)`, then with <i>Nadaraya-Watson Kernel Regression</i>, it can be computed by (if `K` is kernel):</p>
        <p class="formula">&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`f(x) = \sum_{i=1}^n\frac{K(x-x_(i))}{\sum_(j=1)^n K(x-x_(j))}y_(i)`&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;.....<i>eq(4)</i></p>
        <p>This can be further simplified as :</p>
        <p class="formula">&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`f(x) = \sum_{i=1}^n&alpha;(x,x_(i))y_(i)`</p>
        <p>Here, `x` is query and `(x_(i),y_(i))` is key-value pair. Eq(4) is Attention Pooling that provides wegihted avergae of output `y_(i)` by <u>Attention Weight</u>&rarr; `&alpha;(x,x_(i))`</p>
        <p>To get better intuition on Attention Pooling, lets plugin Gaussian kernel in above equation :</p>
        <p class="formula">&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`f(x) = \sum_{i=1}^n&alpha;(x,x_(i))y_(i)` <br>
        &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;&nbsp;&nbsp;`=\sum_{i=1}^n \frac{exp(-\frac{1}{2}(x-x_(i))^2)}{\sum_(j=1)^n&nbsp;exp(-\frac{1}{2}(x-x_(j))^2)}y_(i)`<br>
        &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;&nbsp;&nbsp;`=\sum_{i=1}^n`softmax`(-\frac{1}{2}(x-x_(i))^2)y_(i)`</p>
        <p>This is non-parametric Attention Pooling, we can see that a key `x_(i)` closer to query `x` will be given more attention with large attention weight.</p>
        <p><b>Parametric Attention Pooling</b><br>
        Parametric method is just slightly different from the non-parametric. Here, the distance between the query `x` and key `x_(i)` is multiplied by a learnable parameter `w`. Thus, `f(x)` can be expressed as :</p>
        <p class="formula">&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`f(x) = \sum_{i=1}^n`softmax `(-\frac{1}{2}((x-x_(i))w)^2)y_(i)` </p>
        <p><b>Attention Scoring Function</b></p>
        <p>Exponent of Gaussian Kernel is said to be Attention Scoring Function. In any type of attention pooling, it maps two vectors to scalar, and when computed by softmax operation, it provides attention weight for the query `x` and key `x_(i)`<br>
        If we have a query `q` and `n` key-value pairs `(k_(1),v_(1),...,(k_(n),v_(n)))`, then attention pooling is computed as:</p>
        <p class="formula">&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`f(q(k_(1),v_(1)),...,(k_(n),v_(n))) = \sum_{i=1}^n &alpha;(q,k_(i))v_(i)&isin;&Ropf;^v`</p>
        <p>where attention weight is computed as : <br>
          &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`&alpha;(q,k_(i)) =`softmax`(a(q,k_(i))) = \frac{exp(a(q,k_(i)))}{\sum_(j=1)^n&nbsp;exp(a(q,k_(j)))}&nbsp;&isin;&Ropf;`</p>
        <p>We can observe that different value of attention scoring function &rarr;a can lead to different attention weight, and thus different output for attention pooling. Two popular attention scoring function are :</p>
        <p><b>Additive Attention</b><br>
        Additive Attention is used as scoring function when queries and keys are vectors of different lengthhs. If we have query `q&isin;&Ropf;^q` and a key `k&isin;&Ropf;^k`, the additive attention is expressed as :</p>
        <p class="formula">&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`a(q,k) = w_(v)^T &nbsp;tanh(W_(q)q + W_(k)k)&isin;&Ropf;`</p>
        <p>where learnable parameters `W_(q) &isin;&Ropf;^(h&times;q)`, `W_(k)&nbsp;&isin;&Ropf;^(h&times;k)` and `w_(v) &nbsp;&isin;&Ropf;^h`. Here, `h` is hidden units number</p>
        <p><b>Scaled Dot-Product Attention</b><br>
        Dot-product is used when we have both the query and key of similar vector length `d`. It is further scaled up so that the variance of dot product still remain one by dividing with `\sqrtd`. Scaled Dot-Product Attention scoring function is then : </p>
        <p class="formula">&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`a(q,k) = q^Tk // \sqrtd`</p>
        <p>If we have value as `V` , the attention weight thereafter can be written as :</p>
        <p class="formula">&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;softmax`(\frac{QK^T}{\sqrtd})V`</p>
        <p><b>Multi-Head Attention</b></p>
        <p>Multi-Head Attention allows the attention function to extract information from different representations. It linearly projects the queries, keys and values `h` time, each time using a different learned projection to `d_(k)`,`d_(k)` and `d_(v)` dimensions.</p>
          <img src="assets/img/mh.png" width="300" height="300" class="image_full">
        <p>Then, on each of these projects, we implement <i>Scaled Dot-Product Attention</i> function to gain `d_(v)` dimension output.
        These, afterwards gets concatenated and projected again to provide final result. It can be expressed as :</p>
        <p>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;MultiHead`(Q,K,V) = concat(head_(1),....,head_(h))W^o`</p>
        <p>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;where `head_(i) = `Attention`(QW_(i)^Q,KW_(i)^K,VW_(i)^V)`</p>
        <p>here, the projections are parameter matrices `W_(i)^Q&isin;&Ropf;^(d_(model)&times;d_(k)), W_(i)^K&isin;&Ropf;^(d_(model)&times;d_(k)),W_(i)^V&isin;&Ropf;^(d_(model)&times;d_(k))`, and `W^o&isin;&Ropf;^(hd_(v)&times;d_(model))`</p>
        
        <h2>Transformers</h2>
        <p>Transformers consists of an Encoder-Decoder pair, where each of them is a stack of `L` identical blocks. Encoder block is composed of Multi-Head attention and a position-wise feed-forward network (FFN).
        In addition, a residual connection, followed by Layer Normalization around each module. Decoder block, on the other hand, additionally include cross-attention modules between Multi-Head attention and position-wise FFNs.
        Transformers architecture can be seen below :</p>
        <img src="assets/img/transformer.png" width="480" height="450" class="image_full">
        <p>There are three types of attentions used here in Transformers :</p>
        <p><i>Self-Attention </i>&rarr; It is when queries, keys, and values are generated from the same sequence.<br>
        <i>Cross-Attention </i>&rarr; It is between encoder and decoder, and opposite of self-attention. Here, keys and values are generated by a different sequence than queries. <br>
        <i>Masked Multi-Head Attention </i>&rarr; In transformer decoder side, we apply a mask function to the unnormalized attention matrix `\hatA = exp|frac{QK^T}{\sqrtD_(k)}`, where the illegal positions are masked out by setting `\hatA_(ij) =-&infin;` if `i&le;j`</p>
        <p><b>Position-Wise FFN :</b> The position-wise FFN is a fully connected feed-forward module that operates separately and identically on each position</p>
        <p>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`FFN(H') = ReLU(H'W^1 +b^1)W^2 +b^2 `</p>
        <p><b>Position Encoding :</b>Since Transformers do not have recurrence or convolution, it doesn't really know the which word is at which position. Therefore, position needs to be encoded. One way is to append sine and cosine waves with different frequencies with word vector. This helps transformer to learn the word position because each position have unique combination of values.</p>
        <p>References :</p>
        [1] Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H. and Bengio, Y., 2014. Learning phrase representations using RNN encoder-decoder for statistical machine translation. <i>arXiv preprint arXiv:1406.1078.</i><br>
        [2] Sutskever, I., Vinyals, O. and Le, Q.V., 2014. Sequence to sequence learning with neural networks. <i>Advances in neural information processing systems</i>, 27.<br>
        [3] Bahdanau, D., Cho, K. and Bengio, Y., 2014. Neural machine translation by jointly learning to align and translate. <i>arXiv preprint arXiv:1409.0473.</i>



</body>
</html>
