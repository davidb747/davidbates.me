---
layout: article
title: Object Detection Models - 2
sidebar:
  nav: "docs-en"
---
<!DOCTYPE HTML>
<html>

<!--- Adding Google Analytics -->
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-154990580-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-154990580-2');
</script>
<!-- End of Google Analytics Code -->
<!-- Adding MathJAX -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
  <script async="true" src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=AM_CHTML"> </script>
<!-- End of MathJAX -->


<body>
	<p>Date: 12<sup>th</sup> Dec 2022</p>
	<h2>You Only Look Once (YOLO)</h2>
	<p>YOLO is a single convolutional neural network based model, which unlike, <a href="objmodel.html#rc">R-CNN based models</a>, simultaneously, predicts multiple bounding boxes and its class probabilities of those boxes (Redmon et al., 2016).</p>
	<p>Few advantages are:<br>
	YOLO is extremely fast. As frame detection doesn't go into a complex pipeline, but taken as a regression problem, YOLO gets to process streaming video real-time with less than 25 milliseconds of latency.<br>
	It, unlike sliding window and Region-proposal based technique, uses feature extracted from the entire image, to predict all bounding boxes and its classes.<br>
	YOLO, also, gets to generalize well with unexpected input images/frames - just lags a bit with accuracy.</p>
	<p><b>Detection :</b></p>
	<p>YOLO system divides the input image into an `S&times;S` grid, where each grid cells preditcs `B` bounding boxes. <br>
	Each bounding boxes consists of 5 predictions i.e, `x, y, w, h,` and confidence, where `(x,y)` represent the center of box relative to grid cell. `(w,h)` relative to the whole image.
	Confidence is given by `Pr(Object) &lowast; IOU_(pred)^(truth)` and represent confidence of box containing object and its accuracy.<br>
	Each grid cell also predicts `C` class probabilities `Pr(Class_i | Object)` This is probability of grid containing an object - it predict one class probability per grid cell.</p>
	<p>At test time, both class probabilities gets multiplied 
	`Pr(Class_i | Object) &lowast;Pr(Object) &lowast; IOU_(pred)^(truth)`, providing information on probability of object class in the box, and how well box fits the object.
	</p>
	<p><b>Network Architecture :</b></p>
	<p>Architecture of YOLO is inspired by googLeNet. It has 24 convolutional layers, followed by 2 Fully-connected layers, and instead of inception blocks used in googLeNet, YOLO use `1&times;1` reduction
	layer - to reduce the feature space from preceding layers, followed by `3&times;3` convolutional layer. Network architecture is as shown below:<br>
	&emsp;&emsp;<img src="assets/img/YOLO_arch.png" alt="YOLO Architecture" width="650" height="350" class="image_full"><br>
	During training `224&times;224` resolution image is used, however during detection, `448&times;448` reolution is used for fine-grained visual information. 
	Similarly, linear activation function is used in the final layer, whereas leaky ReLU is used in all the layers prior to it.</p>
	<p><b>Loss Function :</b></p>
	<p>As YOLO detection consists of three phases: 1. detecting bounding box 2. Confidence of box (containing object and its accuracy), and 3. Class probability. Loss Function of YOLO is also have three distinct parts: </p>
	<p><b>1. Localization Loss :</b> In this, as sum-squared error weights error in small and large boxes equally, it can fail to converge quickly, as small deviations in large bouding boxes matter less than small deviations in small bounding boxes. 
	It is addressed by simply using square-root of height and width. Localization Loss can be stated as: </p>
	<p>Localization Loss `= &lambda;_(coord) \sum_(i=0)^(S^2) \sum_(j=0)^B 1_(ij)^(obj) [(x_(ij) - x_(ij)^(gdt))^2 + (y_(ij) - y_(ij)^(gdt))^2 + (sqrt (w_(ij)) - sqrtw_(ij)^(gdt))^2 + (sqrth_(ij) - sqrth_(ij)^(gdt))^2] `</p>
	<p>where,<br>
	`S^2` is the number of grid cells, `B` is the number of bounding boxes per grid cell, `&lambda;_(coord)` is the coefficient to scale the localization loss, and `1_(i)^(obj)` is the indicator function that equals `1` if the `i^(th)` grid cell in the `j^(th)` bounding box contains an object</p>
	<p><b>2. Confidence Loss: </b>It measures the discrepancy between predicted confidence score (of having object in the box) and ground-truth score. However, as grid in the image, may not any object. It can push confidence score of cell to zero. To remedy this, we increase the loss from bounding box coordinate 
	predictions and decrease the loss from conﬁdence predictions for boxes that don’t contain objects, using two parameters i.e, `&lambda;_(coord)` and `&lambda;_(noobj)`, with values `5` and `0.5` respectively. If `C` is the predicted confidence, and `C^(gdt)` is ground-truth confidence, then Confidence Loss then can be expressed as:</p>
	<p>Confidence Loss `=  &lambda;_(coord)  \sum_(i=0)^(S^2) \sum_(j=0)^B 1_(ij)^(obj) (C_(ij) - C_(ij)^(gdt))^2 + &lambda;_(noobj)  \sum_(i=0)^(S^2) \sum_(j=0)^B 1_(ij)^(noobj) (C_(ij) - C_(ij)^(gdt))^2`</p>
	<p><b>3. Classification Loss: </b>It measures the discrepancy between the predicted class probabilities and the ground truth class labels. It is also computer using cross-entropy, and if `p_(i)` is predicted class probability, with `p^(gdt)` as ground-truth class labels, we can write classification loss as :</p>
	<p>Classification Loss `= \sum_(i=0)^(S^2) 1_i^(obj) \sum_(c&isin;classes) (p_(i)(c) - (p_(i)^(gdt)(c))^2)`</p>








	<h2>Retina Net</h2>
	<p><b>Focal Loss:</b></p>
	<p>Focal Loss is designed to address the class imbalance problem, where where the number of background (non-object) samples greatly outweighs the number of object samples</p>
	<p>Focal Loss is based on cross-entroy loss, but designed to down-weight easy (well-classified) examples, and focussing on hard negatives (misclassified) examples during training. 
	It adds a modulating factor `(1 - p_t)^&gamma;` to cross-entroy, and tunable parameter `&gamma; &gt;0`</p>
	<p>For practical purposes, we can write Focal Loss(FL) as :</p>
	<p>&emsp;&emsp;&emsp;`FL(p_t) = -&alpha;_t(1 - p_t)^&gamma;log(p_t)`</p>
	<p>where, `p_t` is the predicted probability of the true class label. In the context of object detection, this probability is obtained from the softmax function applied to the class predictions for each anchor or bounding box. <br>
	`&alpha;_t` is the modulating factor that addresses the class imbalance problems. It is defined as: <br>
	`&alpha;_t = `</p>



<h3>References :</h3>
<ul>
	<li>[1] Redmon, J., Divvala, S., Girshick, R. and Farhadi, A., 2016. You only look once: Unified, real-time object detection. In <i>Proceedings of the IEEE conference on computer vision and pattern recognition </i>(pp. 779-788).</li>
	<li>[2] Lin, T.Y., Goyal, P., Girshick, R., He, K. and Dollár, P., 2017. Focal loss for dense object detection. In <i>Proceedings of the IEEE international conference on computer vision </i>(pp. 2980-2988).</li>
	<li>[3] </li>
	<li>[4] </li>
</ul>
</body>
</html>
