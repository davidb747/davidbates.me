---
layout: article
sidebar:
  nav: "docs-en"
---
<!DOCTYPE HTML>
<html>

<head>
<title>Off-Policy vai Importance Sampling - Git Page - Konark Karna</title>
<meta charset="utf-8"/>
<meta name="author" content="Konark Karna">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="keywords" content="Konark Karna, Konark Karna India, Konark Karna Northumbria University, Konark Karna Newcastle, Konark Karna Computer Science,Data Scientist, Machine Learning Engineer, MSc Advanced Computer Science, Northumbria University,
			       Data Analysis, Data Visualization, Machine Learning, Neural Networks, Deep Learning, Natural Language Processing">
<link rel="stylesheet" type="text/css" href="basic.css">

<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href='http://fonts.googleapis.com/css?family=Play' rel='stylesheet' type='text/css'>
<link href='http://fonts.googleapis.com/css?family=Exo+2:400' rel='stylesheet' type='text/css'>
<link href='http://fonts.googleapis.com/css?family=PT+Sans+Narrow' rel='stylesheet' type='text/css'>
<link href="https://fonts.googleapis.com/css2?family=Cinzel:wght@500&family=Josefin+Sans&family=Balthazar&family=Ropa+Sans&display=swap" rel="stylesheet"> 
</head>
<!--- Adding Google Analytics -->
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-154990580-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-154990580-2');
</script>
<!-- End of Google Analytics Code -->
<!-- Adding MathJAX -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
  <script async="true" src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=AM_CHTML"> </script>
<!-- End of MathJAX -->


<body>
	<header>
		<h1><a href="https://k-karna.github.io/">GitLog</a></h1>
		<section id="nav">
			<ul>
				<li><a href="about.html">about</a></li>
				<li><a href="resume.html">resume</a></li>
				<li><a href="notes.html">AI note-book</a></li>
				<li><a href="contact.html">contact</a></li>
				<li class="b"><a href="https://www.konark.tumblr.com">weblog</a></li>
			</ul>	
		</section>
	</header>
	<section id="text">
		<p class="heading">Off-Policy Prediction via Importance Sampling</p>
        <p class = "dateline"><a href="notes.html"> &lt;&lt; Notes</a> || Date: 30<sup>th</sup> Apr 2022</p>
		<section id = "page">
        <p>All learning contron methods seek to learn action values on optimal behaviour while exploring all actions. One approach for this - is to use two policies, one that is learned about and that becomes the optimal policy, and other that is more
            explotory and is used to generate behavior.</p>
        <p>The policy being learned about is called <b> target policy</b><br>
        The policy that is used to generate behavior is called <b>behaviour policy</b><br>
        Here, as learning is from the data "off" the target policy, we call the process <b>OFF-POLICY LEARNING</b>.</p>
        <p>Prediction Problem of Off-policy can be summarized as: </p>
        <ol>
            <li>if both target and behavior policy are fixed, we need to estimate `v_(pi)` and `q_(pi)`.</li>
            <li>we have all episodes following another policy `b` where `pi` is target policy, `b` is behaviour policy and `b != pi`</li>
            <li>to use episodes from `b` to estimate values for `pi`, every action taken under `pi` is at least taken occasionally under `b` as well. We require, `pi``(a|s)>0` implies `b(a|s)>0`</li>
        </ol>
        <p>This is call <b>assumption of coverage</b>. It follows from coverage that `b` must be stochastic in states where it is not identical to `pi`</p>
        <p>In control, the target policy is typically deterministic greedy policy with respect to the current estimate of the action-value function, while the behavior policy remains stochastic and more explotory like `&epsilon;-greedy`</p>
        <p><b>IMPORTANCE SAMPLING :</b><br> <i> (a technique for estimating expected values under one distribution given samples from another.)</i></p>
        <p>Importance Sampling is applied by weighing returns according to the relative probability of their trajectories occuring under the target and behaviour policies called <b>importance sampling ratio</b></p>
        <p>Given a starting state `S_(t)`, the probability of subsequent state-action trajectory `A_(t), S_(t+1),A_(t+1),S_(t+2),...., S_(T)` occuring under policy `pi`is:<br>
        &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`Pr {A_(t),S_(t+1),A_(t+1),...,S_(T) | S_(t),A_(t:T-1)~ pi}`<br>
        &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`=pi(A_(t)|S_(t))p(S_(t+1)|S_(t),A_(t))pi(A_(t+1)| S_(t+1)) ... p(S_(T) | S_(T-1),A_(T-1))`<br>
        &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`=&Pi;_{k=t}^(T-1) pi (A_(k) |S_(k)) p (S_(k+1)| S_(k),A_(k))`<br>
        where, `p` =state transition probability<br>
        Thus, the importance sampling ratio is:<br>
        &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; `P_(t:T-1) &esdot; \frac{&Pi;_{k=t}^(T-1) pi (A_(k) |S_(k)) p (S_(k+1)| S_(k),A_(k))}{&Pi;_{k=t}^(T-1) b (A_(k) |S_(k)) p (S_(k+1)| S_(k),A_(k))} ` <br>
        &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`=&Pi;_{k=t}^(T-1) \frac {pi (A_(k),S_(k))}{b(A_(k),S_(k))}` &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;....(1)
        </p>
        <p>As numerator and deominator cancels out in eq(1), we can understand importance sampling ratio depends only on policies and sequences.<br>
        In importance sampling, we wish to estimate expected returns under the target policy, but we get `G_(t)` returns due to behaviour policy with expectation i.e., &Eopf;`&#91; G_(t) | S_(t)=s&#93; = V_(b)(s)` which cannot converge to `v_(pi)`.
        Therefore, <i>Importance Sampling</i> that helps find right return is: <br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&Eopf;`&#91;P_(t:T-1) G_(t) | S_(t)=s&#93; = V_(pi)(s)`</p>
		</section>

	</section>

	<footer>
		<p> Follow On: </p>
		<a href="https://www.linkedin.com/in/k-karna/"><img src="images/in.png" alt="linkedin" width="20" height="20"></a>
		<a href= "https://www.github.com/k-karna"><img src="images/gt.png" alt="github" width="20" height="20"> </a>
        <a href= "https://www.twitter.com/konarkkarna"><img src="images/twitter.png" alt="twitter" width="20" height="20"></a>

		<p>Last Updated: Apr 30, 2022<br>
	  </footer>
</body>
</html>
