---
layout: article
title: Off-Policy Prediction via Importance Sampling 
sidebar:
  nav: "docs-en"
---
<!DOCTYPE HTML>
<html>

<!--- Adding Google Analytics -->
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-154990580-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-154990580-2');
</script>
<!-- End of Google Analytics Code -->
<!-- Adding MathJAX -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
  <script async="true" src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=AM_CHTML"> </script>
<!-- End of MathJAX -->


<body>

      <p>Date: 30<sup>th</sup> Apr 2022</p>
      <p>All learning contron methods seek to learn action values on optimal behaviour while exploring all actions. One approach for this - is to use two policies, one that is learned about and that becomes the optimal policy, and other that is more
            explotory and is used to generate behavior.</p>
        <p>The policy being learned about is called <b> target policy</b><br>
        The policy that is used to generate behavior is called <b>behaviour policy</b><br>
        Here, as learning is from the data "off" the target policy, we call the process <b>OFF-POLICY LEARNING</b>.</p>
        <p>Prediction Problem of Off-policy can be summarized as: </p>
        <ol>
            <li>if both target and behavior policy are fixed, we need to estimate `v_(pi)` and `q_(pi)`.</li>
            <li>we have all episodes following another policy `b` where `pi` is target policy, `b` is behaviour policy and `b != pi`</li>
            <li>to use episodes from `b` to estimate values for `pi`, every action taken under `pi` is at least taken occasionally under `b` as well. We require, `pi``(a|s)>0` implies `b(a|s)>0`</li>
        </ol>
        <p>This is call <b>assumption of coverage</b>. It follows from coverage that `b` must be stochastic in states where it is not identical to `pi`</p>
        <p>In control, the target policy is typically deterministic greedy policy with respect to the current estimate of the action-value function, while the behavior policy remains stochastic and more explotory like `&epsilon;-greedy`</p>
        <p><b>IMPORTANCE SAMPLING :</b><br> <i> (a technique for estimating expected values under one distribution given samples from another.)</i></p>
        <p>Importance Sampling is applied by weighing returns according to the relative probability of their trajectories occuring under the target and behaviour policies called <b>importance sampling ratio</b></p>
        <p>Given a starting state `S_(t)`, the probability of subsequent state-action trajectory `A_(t), S_(t+1),A_(t+1),S_(t+2),...., S_(T)` occuring under policy `pi` is:</p>
        <p>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`Pr {A_(t),S_(t+1),A_(t+1),...,S_(T) | S_(t),A_(t:T-1)~ pi}`</p>
        <p>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`=pi(A_(t)|S_(t))p(S_(t+1)|S_(t),A_(t))pi(A_(t+1)| S_(t+1)) ... p(S_(T) | S_(T-1),A_(T-1))`</p>
        <p>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`=&Pi;_{k=t}^(T-1) pi (A_(k) |S_(k)) p (S_(k+1)| S_(k),A_(k))`</p>
        <p>where, `p` =state transition probability<br>
        Thus, the importance sampling ratio is:</p>
        <p>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`P_(t:T-1) &esdot; \frac{&Pi;_{k=t}^(T-1) pi (A_(k) |S_(k)) p (S_(k+1)| S_(k),A_(k))}{&Pi;_{k=t}^(T-1) b (A_(k) |S_(k)) p (S_(k+1)| S_(k),A_(k))} ` </p>
        <p>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`=&Pi;_{k=t}^(T-1) \frac {pi (A_(k),S_(k))}{b(A_(k),S_(k))}` &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;....(1)
        </p>
        <p>As numerator and deominator cancels out in eq(1), we can understand importance sampling ratio depends only on policies and sequences.<br>
        In importance sampling, we wish to estimate expected returns under the target policy, but we get `G_(t)` returns due to behaviour policy with expectation i.e., &Eopf;`&#91; G_(t) | S_(t)=s&#93; = V_(b)(s)` which cannot converge to `v_(pi)`.
        Therefore, <i>Importance Sampling</i> that helps find right return is: <br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&Eopf;`&#91;P_(t:T-1) G_(t) | S_(t)=s&#93; = V_(pi)(s)`</p>


</body>
</html>
