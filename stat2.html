---
layout: article
title: Statistical Tests
date: 2023-04-05
sidebar:
  nav: "docs-en"
---


<html>


<!-- Adding MathJAX -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
  <script async="true" src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=AM_CHTML"> </script>
<!-- End of MathJAX -->


<body>

	<p>Before, <a href="stat2.html#one">One-sample t-test</a>, <a href="stat2.html#two">Two-sampled t-test</a>, <a href="stat2.html#paired">Paired-Sampled t-test</a>, and <a href="stat2.html#anova">ANOVA</a> statistical tests, a few baics :</p>
	<p><b>Null Hypothesis :</b> In statistics, Null Hypothesis is a theory or claim that no relationship exists between two sets of data or variable being analyzed. For a specific characteristic of interest, e.g. mean `&mu;`, of two data sets, null hypothesis can be written as: `H_0: &mu;(&theta;) = &mu;_0`</p>
	<p><b>Alternate Hypothesis :</b> Alternate Hypothesis, claims that a relationship does exists between two data sets, and any difference is not due to chance, but a cause</p>
	<p><b>Test of Significance :</b> It is a statistical procedure being followed in the light of new observation to assess if the hypothesis statement, `H_0: &mu;(&theta;) = &mu;_0`. It is also referred as <i>test of hypothesis</i></p>
	<p><b>P-Value :</b> P-Value is the probability of obtaining results at least as extreme as the results actually observed, assuming the null hypothesis is true. It is used to determine the statistical significance of the test and whether the null hypothesis should be rejected or not.<br>
	The lower the p-value is, the lower the probability of getting that result if the null hypothesis were true, and the result obtained with lower p-value is then said to be <i>statistically significant</i> as it allows us to reject the null hypothesis. `0.05` is generally considered as statistically significant p-value.
	</p>
	<h2><a id="one">One Sample t-test</a></h2>
	<p>One-sample t-test is a statistical test to compare the mean of a single group of data to a known value or hypothesized value of mean of population.<br>
	If we have sample size of `n` observations with `x_1, x_2, ..., x_n` observations in group, `&mu;` as the mean of group, `&mu;_0` is mean of population with null hypothesis, `H_0` stating mean of group, `&mu;` is equal to known/hypothesized mean of population, and 
	alternate hypothesis, `H_a` stating mean of group not equal to mean of population, which can be written as :<br>
	`H_0 : &mu; = &mu;_0`<br> 
	`H_a : &mu; &ne;&mu;_0` <br>
	then to calculate one-sample t-test, first we calculate sample mean, `\tildex` and sample standard deviation, `s` of group<br>
	<p>`\tildex = \frac{\sumX_i}{n}`</p>
	<p>`s = \sqrt\frac{∑(X_i - x̄)²}{n}`</p>
	<p>Next, we calculate t-static as : `t = \frac{\tildex - &mu;_0}{(s//sqrt(n))}`</p>
	<p>Under the null hypothesis, the t-statistic follows a t-distribution with n - 1 degrees of freedom. We can use this distribution to calculate the p-value of the test, which is the probability of obtaining a t-statistic, assuming the null hypothesis is true. 
	If the p-value is less than a pre-specified significance level (e.g., 0.05), we reject the null hypothesis
	</p>
	<h2><a id="two">Two-sampled t-tests (Independent t-test)</a></h2>
	<p>An independent t-test is a statistical test used to compare the means of two independent groups to determine if there is a significant difference between them.</p>
	<p>If we have two groups of data: one with `x_1, x_2, ..., x_(n1)` and size `n1`, and other with `y_1, y_2, ..., y_(n2)` and size `n2` with null hypothesis stating that two groups have the same mean, i.e, `H_0: &mu;_1 = &mu;_2` 
	and the alternative hypothesis is that they have different means i.e, `H_0: &mu;_1 &ne; &mu;_2`
	</p>
	<p>Here, again as in <a href="">one-sample t-test</a> we calculate first mean and standard deviation of both groups as given below:</p>
	<p>`\tildex = \frac{\sumX_i}{n1}`</p>
	<p>`\tildey = \frac{\sumY_i}{n2}`</p>
	<p>`s1 = \sqrt\frac{∑(X_i - x̄)²}{n1}`</p>
	<p>`s2 = \sqrt\frac{∑(Y_i - \tildey)²}{n2}`</p>
	<p>Next, we calculate the pooled standard deviation, which is an estimate of the standard deviation of the population from which the samples were drawn:</p>
	<p>`sp  = \sqrt\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 -2}`</p>
	<p>Finally, t-static is calculated as :</p>
	<p>`t = \frac{\tildex - \tildey}{sp &sdot; \sqrt(\frac{1}{n1} + \frac{1}{n2})}`</p>
	<p>Under the null hypothesis, the t-statistic follows a t-distribution with `(n1 + n2 - 2)` degrees of freedom. We can use this distribution to calculate the p-value of the test, assuming the null hypothesis is true. 
	If the p-value is less than a pre-specified significance level (e.g., `0.05`), we reject the null hypothesis</p>
	<h2><a id= "paired">Paired Sample t-test</a></h2>
	<p>A paired t-test is a statistical test used to compare the means of two measurements taken from the same individual, or related data sets. Here "paired" measurements is to represent things like:<br>
	1. A measurement taken at two different times (e.g, pre-test, post-test)<br>
	2. A measurement taken under two different conditions (e.g, control/experiment)<br>
	3. A measurement taken from the two halves of the same data sets.</p>
	<p>Paired Sample t-test is also knowns as "Dependent t-test", "Paired t-test", "Repeated Measure t-test"</p>
	<p>If we have two sets of measurement groups, both with same size `n`, but with different data points: `x_1, x_2, ..., x_n` for the first set, and `y_1, y_2, ..., y_n` for the second. <br>
	The null hypothesis is that the two sets have the same mean, `H_0: &mu;_1 - &mu;_2 = 0` and the alternative hypothesis is that they have different means. `H_a: &mu;_1 - &mu;_2 &ne; 0`</p>
	<p>Again, we calculate the sample mean, and sample standard deviation of the diffferences of two sets, `d_i = Y_i - X_i`</p>
	<p>`\tilded = \frac{\sumd_i}{n}`</p>
	<p>`s = \sqrt\frac{(\sumd_i - \tilded)^2}{n}`</p>
	<p>Next, we calculate t-static as : `t = \frac{\tilded}{s//\sqrtn}`</p>
	<p>Under the null hypothesis, the t-statistic follows a t-distribution with n - 1 degrees of freedom. We can use this distribution to calculate the p-value of the test, which is the probability of obtaining a t-statistic, assuming the null hypothesis is true. If the p-value is less than a pre-specified significance level (e.g., 0.05), we reject the null hypothesis</p>
	<h2><a id="anova">Analysis of Variance (ANOVA)</a></h2>
	<p>ANOVA is a statistical test used to compare the means of three or more groups. ANOVA is also known as "One Factor ANOVA" or "Between Subjects ANOVA"</p>
	<p>Null Hypothesis in ANOVA is that the means of all groups have the same mean, i.e, `H_0: &mu;_1, = &mu;_2 = ,..., = &mu;_t`. The alternate hypothesis is then, at least one of the population mean is not equal to the others i.e, 
	`H_a: &mu;_i &ne; &mu;_j` for some `i` and `j` where `i &ne; j`</p>
	<p>Test Static for ANOVA:<br>
	For more than two populations, test-static, F, is used. It is a ratio of between group sample variance and the within group sample variance&emsp;<b>`F = `between group variance / within group variance</b><br>
	Under the null hypothesis, ratio between both values should be close to 1, otherwise null hypothesis would get reject.</p>
	<p>For computing F-static, if we have : <br>
	`t` : total number of data groups <br>
	`y_(ij)` : The `j^(th)` observation from the `i^(th)` population<br>
	`n_i` : The sample size from the `i^(th)` population <br>
	`n_T` : The total sample size i.e, `n_T = \sum_(i=1)^t n_i`<br>
	`\tildey_i` : The mean of the sample from the `i^(th)` population<br>
	`\tildey_T` : The mean of the combined data from all the population - overall mean
	</p>
	<p>Then, further we need following before calculating F-static :</p>
	<p>Sum of Squares for Treatment or the Between the Group Sum of Squares <br>
	SST `= \sum_(i=1)^t n_i (\tildey_i - \tildey_T)^2`</p>
	<p>Sum of Squares for Error or the Within Group Sum of Squares <br>
	SSE `= \sum_(i,j)(y_(ij) - \tildey_i)^2` </p>
	<p>Total Sum of Squares :<br>
	TSS `= \sum_(i,j)(y_(ij) - \tildey_T)^2` </p>
	<p>Here, it can be derived as TSS = SST + SSE, now we can set-up ANOVA table to find F-static. ANOVA table is provided below :</p>
	<img src="assets/img/anova-table.png", width="400", height="200", alt="anova-table"><br>
	In the table above, MST is "Mean Square of Treatment", and MSE is "Mean Square of Error" <br>Afterwards, p-value is computed using F-statistic and the F-distribution. If p-value is less than pre-specified significance level (e.g, 0.05), we reject the null hypothesis.



</body>
</html>
