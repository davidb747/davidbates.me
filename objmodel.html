---
layout: article
title: Object Detection Models
sidebar:
  nav: "docs-en"
---
<!DOCTYPE HTML>
<html>

<head>
<title>Two-stage Detectors | Git Page - Konark Karna</title>
<meta charset="utf-8"/>
<meta name="author" content="Konark Karna">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="keywords" content="Convolution Neural Network Convolution Receptive Field Layer Pooling Max pooling Average Pooling Kernel Filter Fully-connected layer ReLU Konark Karna, Konark Karna India, Konark Karna Northumbria University, Konark Karna Newcastle, Konark Karna Computer Science,Data Scientist, Machine Learning Engineer, MSc Advanced Computer Science, Northumbria University,
			       Data Analysis, Data Visualization, Machine Learning, Neural Networks, Deep Learning, Natural Language Processing">
<link rel="stylesheet" type="text/css" href="basic.css">

<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href='http://fonts.googleapis.com/css?family=Play' rel='stylesheet' type='text/css'>
<link href='http://fonts.googleapis.com/css?family=Exo+2:400' rel='stylesheet' type='text/css'>
<link href='http://fonts.googleapis.com/css?family=PT+Sans+Narrow' rel='stylesheet' type='text/css'>
<link href="https://fonts.googleapis.com/css2?family=Cinzel:wght@500&family=Josefin+Sans&family=Balthazar&family=Ropa+Sans&display=swap" rel="stylesheet"> 
</head>
<!--- Adding Google Analytics -->
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-154990580-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-154990580-2');
</script>
<!-- End of Google Analytics Code -->
<!-- Adding MathJAX -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
  <script async="true" src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=AM_CHTML"> </script>
<!-- End of MathJAX -->


<body>

        <p>Date: 13<sup>th</sup> May 2022</p>
	
		<p>Before, <a href="objmodel.html#ss">Selective Search</a>, <a href="objmodel.html#rc">R-CNN</a>, <a href="objmodel.html#frcnn">Fast R-CNN</a>, <a href="objmodel.html#ftr">Faster R-CNN</a> etc, it is essential to first understand the concepts of <a href="objmodel.html#ia">Image Augmentation</a>, <a href="objmodel.html#ft">Fine Tuning</a>, <a href="objmodel.html#ab">Anchor Boxes</a> and <a href="objmodel.html#iou">Intersection over Union(IoU)</a></p>
		<h3><a id="ia">Image Augmentation</a></h3>
		<p>Deep Neural Networks require a large dataset to train on, to get a good generalization ability. <u>Image Augmentation</u> is a technique for that, if the dataset is relatively small.
		Image Augmentation tweaks existing images with few techniques to generate more training images.<br>
		Few techniques are: 1. Flipping/Rotation 2. Cropping 3. Changing Color 4. Rescaling the image 5. Translation - moving image along `x` and `y` axes. 6. Adding <u>Guassian Noise</u> that has zero mean. 7. histogram Equalization etc.
		Keras's <a href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator">ImageDataGenerator</a> is a good tool for this. </p>
		<h3><a id="ft">Fine Tuning</a></h3>
		<p>This is another way of solving the above discussed problem - of lacking more data to train. However, here instead of using more data by image augmentation we use <u>transfer learning</u> to transfer the knowledge of parameters learned on some pre-trained model such as ImageNet.
		Pre-trained models are trained using larger datasets and can help extract image features such as edges, textures, shapes and object composition more effectively.</p>
		<p>This technique is called <u>Fine Tuning</u> and comprised of following steps :<br>
		1. Pre-train a neural network i.e, <i>source model</i> on a source dataset ( e.g, ImageNet)<br>
		2. Create a new neural network i.e, <i>target model</i>. This gets to copy the complete structure and parameters of the source model, except the output layer.<br>
		3. Add an output layer to the target model, whose number of outputs is the number of categories needed and in the target dataset. Then randomly initialize model parameters of this layer.<br>
		4. Train the target model on the target dataset, where the output layer will be trained from scratch <i>(larger learning rate can be used)</i> while all other parameters of previous layers are <i>fine tuned</i> based on the parameters of the source model.<br>
		</p>
		<h3><a id="ab">Anchor Boxes</a></h3>
		<p>Bounding Box, first! It describes the spatial location of an object. It is rectangular and determined by `x` and `y` coordinates of the upper-left corner and lower-right corner.</p>
		<p><u>Anchor Boxes :</u> Object detection algorithms try to sample a large number of regions in an input image to get to as accurately as to <i>ground-truth bounding boxes</i>.
		Every method generate different types of bounding boxes with varying scales and aspect ratios, these generated bounding boxes are called <i>Anchor Boxes</i></p>
		<p><b><u>Generating Multiple Anchor Boxes :</u></b></p>
		<p>Suppose we have an input image that has height `h` and width `w`, and anchor boxes with different shapes are to be generated with <u>scale</u> `s&isin; (0,1]` and <u>aspect ratio <i>(width to height ratio)</i></u> is `r &gt;0`.
		In this case, width of anchor box will be -- `ws\sqrtr` and height -- `hs //\sqrtr`<br>
		If we have series of scales `s_(1),....,s_(n)` and series of aspect ratio `r_(1),...,r_(n)` then, total number of anchor boxes for an input image will be `w&times;h&times;n&times;m` and number of anchor boex centered on a same pixel will be `n + m -1`
		</p>
		<h3><a id ="iou">Intersection over Union (IoU)</a></h3>
		Similar to Jaccard Index that measures similarity between two sets `A` and `B` as `J(A,B) = \frac{A&cap;B}{A&cup;B}`, IoU measures similarity between the anchor box predicted and ground-truth bounding box as shown in (b) below :<br>
		<img src="assets/img/anchor.png" width="750" height="260" alt="anchor box" class="image_full"><br>
		IoU ranges between `0` and `1`, where `0` means no Intersection and `1` means complete
		</p>
		<p><b>Generalized Intersection over Union (GIoU) :</b></p>
		<p>GIOU helps to understand if two shapes are in proximity of each other or very far, if `|A &cap;B| = 0` i.e, IoU` = 0` To find GIoU, we first find the smallest convex shape `C &sube; S &isin; &Ropf;^n` enclosing both `A` and `B`. For example, if `A` and `B` are cube, `C` should be a smallest cube too encompassing them, if `A` and `B` are ellipsoid, `C` should be the smallest ellipsoid.<br>
		Then, we focus on the normalized measure that focuses on the empty volume between `A` and `B`, by calculating a ratio between total volume of C excluding `A` and `B` and divide it up by the total volume of `C`. This gets further substracted from IoU to achieve GIoU.
		(Rezatofighi, H. et al, 2019)</p>
		<p>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`GIoU = IoU - \frac{|C\(A &cup;B)|}{|C|}`</p>
		<p>>> GIoU is just as invariant to the scale of the problem as IoU<br>
		>> Similar to IoU, the value 1 occurs only when two objects overlay perfectly, i.e, if `|A&cap;B| = |A&cup;B|` then `GIoU = IoU = 1`<br>
		>> GIoU value converges to `-1` when the ratio between `|A &cup;B|` and volume of `|C|` tends to zero, i.e, `lim_(\frac{|A&cup;B|}{|C|} &rarr;0) GIoU(AB) = -1`<br>
		`GIoU`, therefore, rectifying weakness of `IoU` can be very useful for 2D/3D tasks</p>
		<p><b>Labelling Anchor Boxes :</b></p>
		<p>Each anchor box has two attributes i.e, <u>class</u> and <u>offset</u>. Class - class of the object appearing in the anchor box. Offset - Offset of the ground-truth bounding box relative to the anchor box.</p>
		<p>Usually, an object detection training set comes with the labels for ground-truth bounding boxes and class of object in it. For prediction, we generate multiple anchor boxes, predict its class and offset to ground-truth boxes, 
		calculate loss, and adjust its position furthermore, eventually leading to bounding boxes that are with `IoU > 0.5` ideally with `0.9 +`
		</p>
		<p><b>Assigning ground-truth bounding boxes to anchor boxes :</b></p>
		<p>
		If we have anchor boxes `A_(1), A_(2), ....., A_(n)` for an image and ground-truth bounding boxes as `B_(1), B_(2),...,B_(n_(b))` where `n_(a) &ge; n_(b)` <br>
		Lets assume a matrix `X &isin; &Ropf;^(n_(a)&times;n_(b))` whose element `x_(ij)` in the `i^(th)` row and `j^(th)` column is the IoU of the anchor box `A_(i)` and the ground-truth bounding box `B_(j)`. The algorithm consists of the following steps :</p>
		<p>
		1. Find the largest element in matrix `X` and denote its row and column indices as `i_(1)` and `j_(1)`, respectively. Then the ground-truth bounding box `B_(j_(1))` is assigned to the anchor box `A_(i_(1))`. Afterwards, all elements in `i_(1)^(th)` row and the `j_(1)^(th)` column in matrix `X` needs to be discarded.<br>
		2. Next, another largest of all the remaining elements in matrix `X` is searched, and its row and column are denoted `i_(2)` and `j_(2)`. Then, we assign ground-truth bounding box `B_(j2)` to anchor box `A_(i2)` and discard all the elements in the `i_(2)^(th)` and `j_(2)^(th)` column in matrix `X`.<br>
		3. Further, we proceed until all elements in `n_(b)` columns in matrix `X` are discarded. Here, we assign a ground-truth bounding box to each of `n_(b)` anchor boxes.<br>
		4. Only traverse through the remaining `n_(a) - n_(b)` anchor boxes. For instance, given any anchor box `A_(i)`, find the ground-truth bouding box `B_(j)` with the largest IoU with `A_(i)` throughout the `i^(th)` row of matrix `X`, and assign `B_(j)` to `A_(i)` only if this IoU is greater than a predefined threshold.
		</p>
		
		</p>
		<h2><a id="ss">Selective Search for Object Recognition</a></h2>
		<p>
		Selective Search method uses hierarchical grouping with a bottom up approach. It first calculates similarity between all neighbouring regions, and groups two most similar regions. Then, this process of finding similarity and grouping is repeated until the whole image becomes a single region.</p>
		<p>Region of an image can not only be formed because of color, and texture, but lighting conditions such as intensity and shading as well contribute to regions.
		Therefore, in the first step, the image is divided into many sub-segments based on color, texture, intensity, etc and then based on similarity measure, two similar regions start to group together until we get the whole image as a region.
		</p>
		<img src="assets/img/ss.png" width="690" height="340" alt="selective search" class="image_full">
		</p>
		<p><b>Similarity Measures :</b></p>
		<p>Similarity is decided with two-pronged approach, first we choose a <u>color space</u> that capture all color spaces with different invariants, and different responses to changes in color, then a similarity metric is choosen for color, texture, size and shape</p>
		<p>For the similarity `s(r_(i),r_(j))` between region `r_(i)` and `r_(j)` following metrics are used :</p>
		<p><u>Color Similarity :</u> With 25 bins for each color channel, a color histogram `C_(i) = {C_(i)^1,..., C_(i)^n}` of each region `r_(i)` with dimensionality `n = 75`. Histogram is normalized with `L_(1)` norm and similarity is measured as :</p>
		<p style="font-size: 18px;">&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`s_(color)(r_(i),r_(j)) = \sum_{k=1}^{n} min(C_(i)^k,C_(j)^k)`</p>
		<p>Color histogram, then, can be effectively propagated through the heirarchy by :</p>
		<p>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`C_(t) = \frac{size(r_(i)) &times; C_(i) + size(r_(j)) &times;C_(j)}{size(r_(i)) + size(r_(j))}` <br>and the size of the resulting region is simply the sum of its constituents : `size(r_(t)) = size(r_(i)) + size(r_(j))`</p>
		<p><u>Texture Similarity :</u> It measures textures with a HOG-like feature extracting Guassian derivatives of the image in 8 directions and for each channel constructing a 10-bin histogram, resulting in a 240-dimensional descriptor.</p>
		<p style="font-size: 18px;">&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`s_{Texture}(r_(i),r_(j)) = \sum_{k=1}^{n} min(t_(i)^k,t_(j)^k)`</p>
		<p><u>Size Similarity :</u> As all small regions needs to be merged into larger ones, we need to add a size component to our metric, that ensures small regions are more similar to each other gets grouped together :</p>
		<p style="font-size: 18px;">&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`s_(size)(r_(i),r_(j)) = 1 - \frac{size(r_(i)) + size(r_(j))}{size(im)}`</p>
		<p>where `size(im)` denotes the size of image in pixels.</p>
		<p><u>Shape Compatibility :</u> Idea, here, is to fill the gaps i.e, if `r_(i)` is contained in `r_(j)` it is logcal to merges these first in order to avoid any holes. OTOH, if `r_(i)` and `r_(j)` are hardly touching each other they should not be merged. If `BB_(ij)` is tight bounding box around `r_(i)` and `r_(j)` then similarity metric can be written as :</p>
		<p style="font-size: 18px;">&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`s_(fill)(r_(i),r_(j)) = 1 - \frac{size(BB_(ij)) - size(r_(i)) - size(r_(j))}{size(im)}`</p>
		<p><b><u>Final similarity Measures :</u></b> It is, then, a combination of all the above four metrics as :</p>
		<p style="font-size: 20px;">&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`s_(r_(i), r_(j)) = a_(1)s_(color)(r_(i), r_(j)) + a_(2)s_(Texture)(r_(i), r_(j)) + a_(3)s_(size)(r_(i), r_(j)) + a_(4)s_(fill)(r_(i),r_(j))`</p>
		where `a_(i) &isin;{0,1}` denotes if the similarity measure is used or not.
		<p><b>Evaluation :</b></p>
		<p>Evaluation is done by Average Best Overlap (ABO) and Mean Average Best Overlap. To calculate ABO for a specific class `c`, we calculate the best overlap between each ground truth annotation `g_(i)^c &isin;G^c` and the object hypotheses `L` generated for the corresponding image.</p>
		<p>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`ABO = \frac{1}{|G^c|}\sum_(g_(i)^c &isin;G^c)&nbsp;max` Overlap`(g_(i)^c,l_(i))`<br>
		where Overlap score measure the area of the intersection of two regions divided by its union :<br>
		&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;Overlap`(g_(i)^c,l_(i)) = \frac{area(g_(i)^c)&cap;area(l_(j))}{area(g_(i)^c)&cup;area(l_(j))}`</p>
		<h2><a id ="rc">R-CNN</a></h2>
		<p>R-CNN, first, extract the <u>region proposals</u> from the input image and label its class and bounding box, then a CNN is implemented to extract its feature. These features gets further used with Support Vector Machine (SVM) to finally predict its class and the bounding box (Girshick et al., 2014).
		</p>
		<p>Object detection modelling with R-CNN can be done in following steps : </p>
		<p>1. Perform <a href="objmodel.html#ss">selective search</a> on images to extract multiple region proposals on input images. These are usually selected at multiple scale with different shapes and sizes. And, each region proposal will be labeled with a class and a ground-truth bounding box. (Girshick et al., 2014) extracted nearly 2000 region proposals.</p>
		<p>2. Select a pre-train model such as <a href="cnn_dp.html#an">AlexNet</a>, and remove the output layer. Resize each region proposal to the input size required by the network, and output the extracted features for the region proposal through forward propagation.<br>
		All region proposal with the `IoU &ge;0.5` is treated as positive whereas rest of region proposals are treated as negative, then SGD is used with learning rate of 0.001</p>
		<img src="assets/img/rcnn.png" width="590" height="190" alt="rcnn" class="image_full"><br>
		<p>3. Next for each class, we determine with each extracted feature vector, using SVM trained for that class, if that region contain a specific class or not. For partial overlaps, a threshold of 0.3 is used, below which all are labeled negative, and as traning data is too large to fit in, a standard hard mining method is used to converge quickly.</p>
		<p>4. Finally, a bounding box regressor is used to predict the ground-truth bounding boxes</p>
		<h2><a id="frcnn">Fast R-CNN</a></h2>
		<p>
		Fast R-CNN is comparatively faster to train and test than <a href="objmodel.html#rc">R-CNN</a> as it takes the entire image as input in the network (Girshick, 2015).</p>
		<p><b>Fast R-CNN architecture consists of following steps :</b></p>
		<p><b>1.</b> The Fast R-CNN network first processes the whole image with several convolutional and max pooling layers, and produces a feature map. <br>
		<b>2.</b> Then, using <a href="objmodel.html#ss">selective search</a> on input image, `n` region proposals are generated.<br>
		<b>3. </b>Next, for each of `n` region proposals, a region of interest(RoI) pooling layer extracts a fixed length feature vector from the received feature-map.<br>
		<p><b>Region of Interest (RoI) pooling layer :</b></p>
		<p>RoI pooling layer is slightly different from conventional pooling layer, here we can directly specify the output shape `H&times;W`. It then uses max pooling to convert the feature inside a region of interest into a small feature map.<br>
		Each RoI is defined by a four-tuple `(r,c,h,w)` that specifies its top-left corner `(r,c)` and its height and width `(h,w)`<br>
		Afterwards, RoI max pooling works by dividing the `h&times;w` RoI window into an `H&times;W` grid of sub-windows of approximate size `h//H &times; w//W` and then max-pooling the values in each sub-window into the corresponding output grid cell.<br>
		<img src="assets/img/roi.png" width="430" height="130" alt="Region of Interest" class="image_full"><br>
		Here, height and width of any subwindow gets rounded up and the largest element is used as the output<br>
		</p>
		<b>4.</b> Afterwards, any of the <a href="cnn_dp.html">pre-trained models</a> with `5` max pooling layer and between `5` and `13` conv layers is used with three changes : <br>
		<b>(a) </b>The last max pooling layer is replaced with a RoI pooling layer by configuring `H&times;W` size is compatible with the pre-trained models first fully connected layer.<br>
		<b>(b) </b>Pre-trained models last fully connected layer and softmax is replaced with two sibling layers, where <i>first</i> produces softmax probability estimates over `K` object classes plus a catch-all 'background', essentially `K+1` object classes, and the <i>second</i> layer produces four numbers for each of the `K` object classes. Each set of 4 values defines bounding-box position for each of the `K` classes except 'background'.<br>
		<b>(c) </b>Pre-trained model is modified to take two data inputs : a list of images, and a list of RoIs in those images.<br>
		<img src="assets/img/frcnn.png" width="530" height="170" alt="Fast R-CNN architecture" class="image_full"><br>
		<b>Multi-Task Loss :</b> 
		<p>A Fast R-CNN network produces two sibling output layers. One provides a discrete probability distribution (per RoI), `p = (p_(0), ... ,p_(k))` over `K+1` categories. Another layer provides bounding-box regression offstes, `t^(k) = (t_(x)^k, t_(y)^k, t_(w)^k, t_(h)^k)` where `t^k` specifies a scale-invariant translation and log-space height/width shift relative to region proposal.</p>
		<p>If each training RoI is labeled with a ground-truth class `u` and a ground-truth bounding box regression target is `v`, then multi-task loss `L` on each labeled RoI is as :</p>
		<p style="font-size: 18px;">&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`L(p,u,t^u,v) = L_(cls)(p,u) + &lambda;[u&ge;1] L_(loc)(t^u,v)`</p>
		<p>in which `L_(cls)(p,u) = - log p_(u)` is log loss for true class `u`.<br>
		Here, hyper-parameter `&lambda;` is to control the balance between the two tasks losses.<br>
		The Iverson bracket indicator function `[u&ge;1]` evaluates to 1 when `u &ge;1` and otherwise `0`<br>
		The second task loss,`L_(loc)` is defined over a tuple of true bounding-box regression targets for class `u, v = (v_(x), v_(y), v_(w), v_(h))` and a predicted tuple `t^u = (t_(x)^u, t_(y)^u, t_(w)^u, t_(h)^u)` again for class `u`. For bounding-box regression, we use the loss :</p>
		<p>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`L_(loc)(t^u, v) = \sum_{i&isin;x,y,w,h} smooth_(L_(1))(t_(i)^u - v_(i))`</p>
		<p>where `smooth_(L_(1))(x) = 0.5x^2` if `|x| &lt;1` and `|x| - 0.5` otherwise</p>
		<h3><a id="ftr">Faster R-CNN</a></h3>
		<p>Faster R-CNN replaces selective search with a <i>Region Proposal Network (RPN)</i>. It helps reduce region proposals without loss in accuracy for object detection.(Ren et al., 2015)</p>
		<p><b>Region Proposal Networks (RPNs) :</b></p>
		<p>A RPN takes an input image and outputs a set of rectangular object proposals, each with an <i>objectness score</i>.
		It is constructed by adding two additional conv layers: one that encodes each conv map position into a short feature vector and a second that, at each conv map position, outputs an objectness score, and regressed bounds for `k` region proposal.</p>
		<p>To generate Region Proposal, we slide a small network over the conv feature map output of the first CNN, it is fully-connected to an `n&times;n` spatial window of input conv feature map. Each sliding window is then mapped to a lower-dimension vector, which is fed into two sibling fully-connected layers - box-regression layer <i>(reg)</i> and box-classification layer <i>(cls)</i>.<br>
		<img src="assets/img/faster-rcnn.png" width="560" height="300" alt="faster-RCNN" class="image_full">
		</p>
		<p>Then, at each sliding-window location, we get `k` region proposal where <i>reg</i> layer provides `4K` output (coordinates of `k` boxes) and <i>cls</i> layer provides `2k` score estimating probability of <u>object / not-object</u> for each proposal.<br>
		These `k` proposals are parameterized relative to `k` reference boxes called <i>Anchors</i>. With k = 9 anchor boxes at each sliding position.</p>
		<p>Importantly, some RPN proposals highly overlap each other. Therefore, non-maximum suppression (NMS) is adopted on the regions proposals based on their <i>objectness-score</i>,, with IoU threshold fixed at 0.7. NMS helps remove a substantial number of proposals.</p>


		<p><b>References :</b></p>
		<p>
		<ul>
			<li>[1] Rezatofighi, H., Tsoi, N., Gwak, J., Sadeghian, A., Reid, I. and Savarese, S., 2019. Generalized intersection over union: A metric and a loss for bounding box regression. In <i>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</i> (pp. 658-666). </li>
			<li>[2] Girshick, R., Donahue, J., Darrell, T., & Malik, J. (2014). Rich feature hierarchies for accurate object detection and semantic segmentation. <i>Proceedings of the IEEE conference on computer vision and pattern recognition</i> (pp. 580–587).</li>
			<li>[3] Girshick, R. (2015). Fast r-cnn. <i>Proceedings of the IEEE international conference on computer vision</i> (pp. 1440–1448).</li>
			<li>[4] Ren, S., He, K., Girshick, R., & Sun, J. (2015). Faster r-cnn: towards real-time object detection with region proposal networks. <i>Advances in neural information processing systems</i> (pp. 91–99).</li>
		</ul>
		</p>
		</p>

</body>
</html>
