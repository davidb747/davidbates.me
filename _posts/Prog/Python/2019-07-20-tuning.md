---
layout: article
title: Hyperparameter tuning
tags: Python
sidebar:
  nav: docs-en
---

모델을 선택한 다음 해야할 일은 모델의 hyperparameter를 정하는 **hyperparameter tuning** 과정입니다. <br>

<!--more-->

---

Parameter를 학습시킬 땐 **train data**를 사용하여 score를 비교하여 가장 높은 score를 보여준 parameter를 선택하였습니다. 적절한 hyperparameter를 찾기 위해서는 overfitting을 막기 위해 train data로 분리된 **validation data**를 사용하는데요, 이 validation data를 나누고 tuning 하는 방법은 어떤 모델을 사용하는지에 따라 크게 2가지로 구분할 수 있습니다. <br>

### 1. <학습 - 예측 - score 계산> 과정의 시간이 오래 걸리는 경우
Deep learning과 같이 많은 양의 데이터를 다루거나 학습시키고 예측하여 score를 계산하는 시간이 오래 걸리는 경우, 처음 data를 나눌 때 서로 격리된 **train / validation / test data** 로 나눕니다. <br>
Train data와 마찬가지로 **validation data에서 발생하는 cost가 test data에서 발생하는 cost와 유사할 것이다** 라는 전제하에 validation data에서 가장 좋은 성능을 보여주는 hyperparameter를 선택할 수 있습니다.


### 2. <학습 - 예측 - score 계산> 과정의 시간이 오래 걸리지 않는 경우
또다른 방법은 deep learning 같이 대용량 데이터를 처리하는 방식에서는 엄두도 내지 못할만한 방법인데요. <br>
이번에는 **validation data**를 정해놓지 않고 데이터를 **train / test data**로 나눕니다. Train data를 동일한 크기를 가진 k개의 fold로 분할한 다음, 각각의 fold를 validation data로, 그 외의 (k-1)개의 folds를 train data로 하는 모델(모두 동일한 hyperparameter)을 k개 만들 수 있습니다. k번 학습한 결과 score의 평균을 구하고 이 과정을 각각의 hyperparameter 마다 반복하여 비교할 수 있습니다. 이러한 방법을 **k-fold cross validation**이라고 합니다. <br>
Cross validation은 하나의 hyperparameter를 평가하기 위해 서로 다른 train / validation data에 대한 k개의 모델을 학습시키고 그 평균값으로 평가하기 때문에 hyperparameter의 일반화 성능에 대한 신뢰성이 높을 뿐만 아니라, validation data가 고정되어 있지 않아 train data의 효율성을 극대화하는 방법으로 자주 사용되는 방법입니다.

*sklearn*에서는 기본적인 estimator를 사용한다면 정말 간단하게 cross validation을 사용할 수 있습니다. <br>
