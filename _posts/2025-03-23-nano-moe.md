---
layout: article
title: "NanoMoE: Extending NanoGPT with Mixture of Experts"
tags: nlp deep-learning llm
---

# Intro

Mixture of Experts (MoE) has become increasingly popular in the LLM community, and for good reason. These models effectively maintain the scaling laws originally established for dense models while keeping inference costs relatively manageable – a compelling advantage in our era of ever-growing language models. To better understand how MoE architectures actually work under the hood, I decided to extend Andrej Karpathy's awesome [nanoGPT](https://github.com/karpathy/nanoGPT) repository to support MoE architecture.

The modification turned out to be quite straightforward: I adapted the feed-forward network (FFN) component of the GPT-2 architecture to use the Mixture of Experts approach and ran a series of small-scale pre-training experiments to gain some hands-on insights about training MoE models. Through these experiments, I was able to verify some of the core claims from Google's seminal 2021 paper [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961).

# Understanding MoE Architecture

Before diving into implementation details, let's clarify what MoE actually is. In a standard transformer model, each layer processes all tokens through the same parameters. In an MoE model, we replace certain components (typically the feed-forward networks) with a collection of "expert" networks. A routing mechanism then decides which expert(s) should process each token.

The beauty of this approach is that for each input token, we only activate a small subset of the total parameters, creating sparsity during both training and inference. This sparsity allows us to scale to much larger parameter counts without proportionally increasing computation costs.

![moe-diagram](/assets/images/posts/nanomoe/switch-transformers-diagram.png)

The key components of the MoE Architecture include:

- **Experts**: Specialized neural networks (in our case, feed-forward networks) that each focus on different aspects of the input. For our implementation, we simply use the original FFN of GPT-2 for each expert.
- **Router**: A lightweight neural network that determines which expert(s) should process each token. In our implementation, this is a simple softmax layer.
- **Gating Mechanism**: Determines how to combine outputs when multiple experts process the same token (in some MoE variants). The gating mechanism in my implementation is top-k selection.

# Implementing MoE in nanoGPT

NanoGPT provides a clean, minimalist implementation of the GPT-2 architecture, making it an ideal foundation for experimentation. Here's how I extended it to support MoE:

```python
class MoELayer(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)
        self.attn = CausalSelfAttention(config)
        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)
        # normal transformers just have a MLP here
        # self.ffn = MLP(config)
        self.moe_ffn = MoEBlock(config)

    def forward(self, x):
        x = x + self.attn(self.ln_1(x))
        moe_output, routing_weights = self.moe_ffn(self.ln_2(x))
        x = x + moe_output
        return x, routing_weights


class MoEBlock(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.n_embd = config.n_embd
        self.num_experts = config.num_experts
        self.top_k = config.top_k
        
        self.dropout = nn.Dropout(config.dropout)
        self.gate = nn.Linear(self.n_embd, self.num_experts, bias=False)
        self.experts = nn.ModuleList([MLP(config) for _ in range(self.num_experts)])

    def forward(self, x):
        batch_size, sequence_length, hidden_dim = x.shape
        # reshape x into 2-dimensional tensor for easier indexing across tokens
        x = x.view(-1, hidden_dim)  # (batch * seq, hidden_dim)
        router_logits = self.gate(x)  # (batch * seq, num_experts)
        routing_weights = F.softmax(router_logits, dim=-1, dtype=torch.float)
        routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)
        # routing_weights (batch * seq, top_k)
        # selected_experts (batch * seq, top_k)
        # re-normalize the routing weights after top-k selection
        routing_weights /= routing_weights.sum(dim=-1, keepdim=True)
        # cast back to the input dtype
        routing_weights = routing_weights.to(x.dtype)

        # initialize an empty tensor to accumulate the output of each expert
        total_output_states = torch.zeros(
            (batch_size * sequence_length, hidden_dim), dtype=x.dtype, device=x.device
        )

        # One-hot encode the selected experts to create an expert mask
        # this will be used to easily index which expert is going to be solicited
        expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)
        # (batch * seq, top_k, num_experts) => (num_experts, top_k, batch * seq)
        # expert_mask[expert_idx][top_k_idx][token_idx] is 1 if the expert_idx-th expert is turned on
        # top_k_idx:  0 ... top_k
        # token_idx: token 0 ... batch_size * seq_length

        # Loop over all available experts in the model and perform the computation on each expert
        for expert_idx in range(self.num_experts):
            expert_layer = self.experts[expert_idx]
            top_k_idx, token_idx = torch.where(expert_mask[expert_idx])

            # current state: all token inputs related to this expert
            # Add None in order to broadcast
            current_state = x[None, token_idx].reshape(-1, hidden_dim)
            # forward the expert and then multiply by the weights
            current_hidden_states = expert_layer(current_state) * routing_weights[token_idx, top_k_idx, None]

            # Accumulate the expert output in the total_output_states buffer
            total_output_states.index_add_(0, token_idx, current_hidden_states.to(x.dtype))

        total_output_states = total_output_states.reshape(batch_size, sequence_length, hidden_dim)
        
        return total_output_states, router_logits
```

# Experimental Setup

To validate the MoE implementation and compare it with the standard GPT-2 architecture, I ran a series of experiments with the following configurations:

- **Dense Model Baselines**
    - **GPT-2 Small**: Standard GPT-2 small (124M parameters)
    - **GPT-2 Medium**: Standard GPT-2 medium (353M parameters)
    - **GPT-2 Large**: Standard GPT-2 large (774M parameters)
- **MoE Models**
    - **MoE-2**: GPT-2 small with 2 experts (180M parameters)
    - **MoE-4**: GPT-2 small with 4 experts (293M parameters)
    - **MoE-8**: GPT-2 small with 8 experts (520M parameters)
    - **MoE-16**: GPT-2 small with 16 experts (973M parameters)

I'm running these experiments on fairly modest hardware – just 8x V100-32G GPUs. Without access to more substantial compute resources, I couldn't run the GPT-2 XLarge variant or scale up my MoE experts further without implementing distributed training with Model Parallelization (which would significantly complicate the code).

For all experiments, I used consistent hyperparameters, keeping almost everything aligned with the original NanoGPT settings:

- **Dataset**: the OpenWebText corpus
- **Total Batch size**: ~0.5M tokens (adjusting local batch size and gradient accumulation steps across different models to maintain consistency)
- **Learning rate**: 6e-4 with cosine decay
- **Training steps**: 10,000

This means all models were pretrained on approximately 5B tokens. While this is relatively modest (about 1/60 of the training tokens that Andrej used for his GPT-2 small experiments), it's sufficient to observe the patterns I'm interested in.

For context, here's how this compares to the training compute used for modern LLMs:

| Model                | Parameters | Training Tokens      |
|----------------------|------------|----------------------|
| GPT-2 Small (Mine)   | 124M       | 5B (val loss 3.15)   |
| GPT-2 Small (Karpathy) | 124M     | 300B (val loss 2.85) |
| LLaMA 1              | 7-65B      | 1-1.4T               |
| LLaMA 2              | 7-70B      | 2T                   |
| LLaMA 3              | 8-405B     | 16T                  |
| Qwen 2.5             | 0.5-70B    | 18T                  |

# Results

### 1. MoE models can be scaled up by adding more experts while keeping model FLOPS constant

This is the core advantage of MoE that seems almost too good to be true. With the same model FLOPS, the inference cost stays constant (achieving this actually takes significant engineering effort) but the model performance continues to improve as we add more experts.

When you think about it, this makes perfect sense. Due to the sparse architecture, MoE effectively disentangles the number of model parameters from the model FLOPS. As we add more experts, the parameter count increases while computational requirements remain stable. So it's no surprise that model performance improves, aligning perfectly with established scaling laws.

![](/assets/images/posts/nanomoe/switch_transformer_scaling_moe_plot.png)

*Figure 1: Performance scaling of Switch Transformer models with increasing expert count*

![](/assets/images/posts/nanomoe/scaling_moe_plot.png)

*Figure 2: Performance scaling of GPT2-MoE models with increasing expert count*

From *Figure 1* (from the original Switch Transformers paper), you can see how scaling T5-base to 16, 32, 64, and 128 experts leads to corresponding performance gains. My experiments, shown in *Figure 2*, tell the same story – the loss curves get progressively lower as we add more experts to GPT2-small. While the metrics differ between the two plots (negative log perplexity vs. raw loss), the trend is identical: more experts yield better performance at the same computational cost.

## 2. MoE models achieve comparable performance to larger dense models with significantly lower FLOPS

![](/assets/images/posts/nanomoe/switch_transformer_dense_vs_moe.png)

*Figure 3: Switch Transformer with the same FLOPS as T5-base matches the performance of T5-large (3.5x the FLOPS of T5-base)*

Another compelling insight from the Switch Transformer paper is that a computationally-equivalent MoE model substantially outperforms its dense counterpart. As demonstrated in Figure 3 above, a model using T5-base architecture with 64 MoE experts not only surpasses T5-base itself but can match the performance of T5-large – which requires 3.5x the computational resources.

My experiments with GPT-2 show similar results. In Figure 2, you can see that as we increase the number of experts, our MoE variants of GPT2-small approach the performance of GPT2-medium (which requires 3.5x the FLOPS of GPT2-small). While my largest experiment with 16 experts doesn't quite reach GPT2-medium performance levels, the trend is clear. Extrapolating from these results, we can reasonably predict that scaling to 32 or 64 experts would likely match or exceed GPT2-medium's performance – all while maintaining the computational efficiency of the smaller model.

## 3. Parameter Efficiency: MoE models require significantly more parameters than dense models for equivalent performance

A corollary to point #2 is that when comparing models with similar parameter counts (rather than FLOPS), MoE models actually underperform their dense counterparts by a substantial margin. As Figure 2 and the table below demonstrate, even though GPT-2 small with 8 and 16 experts has more parameters than GPT-2 medium, it still doesn't match the dense model's performance:

| Model                  | Parameters | Val loss after 10k steps |
|------------------------|------------|--------------------------|
| GPT-2 Small            | 124M       | 3.151                    |
| GPT-2 Small 4 experts  | 293M       | 3.076                    |
| GPT-2 Small 8 experts  | 520M       | 3.036                    |
| GPT-2 Small 16 experts | 973M       | 3.021                    |
| GPT-2 Medium           | 353M       | 2.955                    |

This raises an interesting question: how many parameters would an MoE model need to match a dense model's performance? Using the findings from the Switch Transformer paper (where T5-base with 64 experts matches T5-large's performance), we can estimate that it would take GPT2-small with 64 experts to match GPT2-medium's performance. That would translate to approximately 3.6B parameters - an order of magnitude more than GPT2-medium's 353M parameters.

This highlights the fundamental trade-off of MoE architectures: they offer remarkable computational efficiency but require substantially more parameters to achieve the same performance as dense models. This parameter inefficiency is the price we pay for the computational benefits - a worthwhile exchange in many practical scenarios where inference costs or latency are primary concerns.

## 4. 

## 5. Load Balance Loss


结论 4：在相同参数量下，MoE 模型可以通过增加 top_k 或者 shared expert 来提高模型的效果

虽然总参数量不变，但是增加 top_k 或者 shared expert 能够增加模型的 active params, 即推理 FLOPS，模型效果能够提升

为什么说 MoE 训练难
大家都说MoE训练不稳定，来自 expert 的 balance。是真的吗？多不平衡会影响到训练?
Load Balance Loss 的定义
$loss = \alpha * N * \sum_{i=1}^N f_{i} * P_{i}$

$f_i = $分给 expert_i 处理的 tokens 比例  (受到 top_k 截断影响)
$P_i =$分到 expert_i 的路由概率 (router probability), 不受到 top_k 截断影响

通过可视化理解理论和实际情况下的load balance loss
情况
可视化
loss
所有 token 完美平均分配

$min \ loss = N * \sum_{i=1}^N \frac{1}{N} * \frac{1}{N} = 1$
所有 token 都分给同一个 expert

$max \ loss = N * 1 * 1 = 8$
一个 expert 被完全忽略

$loss = 8 * \sum_{i=1}^{7}\frac{1}{7} * \frac{1}{7} = 1.142857$
2个 expert 被完全忽略

$loss = 8 * \sum_{i=1}^{6}\frac{1}{6} * \frac{1}{6} = 1.333$
2个 expert 的概率略微少了，2个 expert 的概率略微大了

$loss = 8 * (0.125 * 0.125 * 4 + 0.15 * 0.15 * 2 + 0.1 * 0.1 *2)  = 1.02$
4个 expert 的概率略微少了，4个 expert 的概率略微大了

$loss = 8 * (0.15 * 0.15 * 4 + 0.1 * 0.1 *4)  = 1.04$

结论 1：expert 越多，越容易出现 expert imbalance

expert 越多，load balance loss 的理论最大值也越大。实际看来，也越容易出现不平衡的现象。
结论 2：通过加 load balance loss 可以有效提升 expert balance, 平衡的模型略微优于不平衡（但稳定）的模型

其实，不平衡但是 expert balance 稳定的模型，效果并不差，训练也比较平稳。
结论 3：稳定的 expert imbalance 并不太影响训练的稳定性(影响模型的上限)，影响最大的是突然大量 token 换 expert



如果训练初期出现这种问题，一般 loss 曲线 recover 的还比较快。但是训练中后期模型的 expert 已经适应了数据分布(loss 较低)，如果 router 层突然有大量 token 更改 expert，那么 loss 会有很大的 jump，需要比较久才能 recover。
top_k = 1 的时候受这种现象影响比较严重 (每个 token 仅有 1 个 expert 点亮)，而且不管有没有打开 load balance loss 都有这种现象。
也许这就是为什么一般来说 top_k > 1, 或者需要有 shared expert 的原因，不能把鸡蛋都放在同一个篮子里。