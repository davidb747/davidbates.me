---
layout: article
title: Object Detection Models - 2
date: 2022-12-12
sidebar:
  nav: "docs-en"
---
<!DOCTYPE HTML>
<html>

<!--- Adding Google Analytics -->
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-154990580-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-154990580-2');
</script>
<!-- End of Google Analytics Code -->
<!-- Adding MathJAX -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
  <script async="true" src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=AM_CHTML"> </script>
<!-- End of MathJAX -->


<body>
	<a href="objmodel1.html#yolo">1. YOLO</a>, <a href="objmodel1.html#rt">2.Retina Net</a>, and <a href="objmodel1.html#ssd">3. SSD</a> 
	<h2><a id = "yolo">You Only Look Once (YOLO)</a></h2>
	<p>YOLO is a single convolutional neural network based model, which unlike, <a href="objmodel.html#rc">R-CNN based models</a>, simultaneously, predicts multiple bounding boxes and its class probabilities of those boxes (Redmon et al., 2016).</p>
	<p>Few advantages are:<br>
	YOLO is extremely fast. As frame detection doesn't go into a complex pipeline, but taken as a regression problem, YOLO gets to process streaming video real-time with less than 25 milliseconds of latency.<br>
	It, unlike sliding window and Region-proposal based technique, uses feature extracted from the entire image, to predict all bounding boxes and its classes.<br>
	YOLO, also, gets to generalize well with unexpected input images/frames - just lags a bit with accuracy.</p>
	<p><b>Detection :</b></p>
	<p>YOLO system divides the input image into an `S&times;S` grid, where each grid cells preditcs `B` bounding boxes. <br>
	Each bounding boxes consists of 5 predictions i.e, `x, y, w, h,` and confidence, where `(x,y)` represent the center of box relative to grid cell. `(w,h)` relative to the whole image.
	Confidence is given by `Pr(Object) &lowast; IOU_(pred)^(truth)` and represent confidence of box containing object and its accuracy.<br>
	Each grid cell also predicts `C` class probabilities `Pr(Class_i | Object)` This is probability of grid containing an object - it predict one class probability per grid cell.</p>
	<p>At test time, both class probabilities gets multiplied 
	`Pr(Class_i | Object) &lowast;Pr(Object) &lowast; IOU_(pred)^(truth)`, providing information on probability of object class in the box, and how well box fits the object.
	</p>
	<p><b>Network Architecture :</b></p>
	<p>Architecture of YOLO is inspired by googLeNet. It has 24 convolutional layers, followed by 2 Fully-connected layers, and instead of inception blocks used in googLeNet, YOLO use `1&times;1` reduction
	layer - to reduce the feature space from preceding layers, followed by `3&times;3` convolutional layer. Network architecture is as shown below:<br>
	<img src="assets/img/YOLO_arch.png" alt="YOLO Architecture" width="650" height="350" class="image_full"><br>
	During training `224&times;224` resolution image is used, however during detection, `448&times;448` reolution is used for fine-grained visual information. 
	Similarly, linear activation function is used in the final layer, whereas leaky ReLU is used in all the layers prior to it.</p>
	<p><b>Loss Function :</b></p>
	<p>As YOLO detection consists of three phases: 1. detecting bounding box 2. Confidence of box (containing object and its accuracy), and 3. Class probability. Loss Function of YOLO is also have three distinct parts: </p>
	<p><b>1. Localization Loss :</b> In this, as sum-squared error weights error in small and large boxes equally, it can fail to converge quickly, as small deviations in large bouding boxes matter less than small deviations in small bounding boxes. 
	It is addressed by simply using square-root of height and width. Localization Loss can be stated as: </p>
	<p>Localization Loss `= &lambda;_(coord) \sum_(i=0)^(S^2) \sum_(j=0)^B 1_(ij)^(obj) [(x_(ij) - x_(ij)^(gdt))^2 + (y_(ij) - y_(ij)^(gdt))^2 + (sqrt (w_(ij)) - sqrtw_(ij)^(gdt))^2 + (sqrth_(ij) - sqrth_(ij)^(gdt))^2] `</p>
	<p>where,<br>
	`S^2` is the number of grid cells, `B` is the number of bounding boxes per grid cell, `&lambda;_(coord)` is the coefficient to scale the localization loss, and `1_(i)^(obj)` is the indicator function that equals `1` if the `i^(th)` grid cell in the `j^(th)` bounding box contains an object</p>
	<p><b>2. Confidence Loss: </b>It measures the discrepancy between predicted confidence score (of having object in the box) and ground-truth score. However, as grid in the image, may not any object. It can push confidence score of cell to zero. To remedy this, we increase the loss from bounding box coordinate 
	predictions and decrease the loss from conﬁdence predictions for boxes that don’t contain objects, using two parameters i.e, `&lambda;_(coord)` and `&lambda;_(noobj)`, with values `5` and `0.5` respectively. If `C` is the predicted confidence, and `C^(gdt)` is ground-truth confidence, then Confidence Loss then can be expressed as:</p>
	<p>Confidence Loss `=  &lambda;_(coord)  \sum_(i=0)^(S^2) \sum_(j=0)^B 1_(ij)^(obj) (C_(ij) - C_(ij)^(gdt))^2 + &lambda;_(noobj)  \sum_(i=0)^(S^2) \sum_(j=0)^B 1_(ij)^(noobj) (C_(ij) - C_(ij)^(gdt))^2`</p>
	<p><b>3. Classification Loss: </b>It measures the discrepancy between the predicted class probabilities and the ground truth class labels. It is also computer using cross-entropy, and if `p_(i)` is predicted class probability, with `p^(gdt)` as ground-truth class labels, we can write classification loss as :</p>
	<p>Classification Loss `= \sum_(i=0)^(S^2) 1_i^(obj) \sum_(c&isin;classes) (p_(i)(c) - (p_(i)^(gdt)(c))^2)`</p>
	<p>Total Loss, thereafter, is the sum of all three losses, i.e,</p>
	<p>Total Loss = Localization Loss + Confidence Loss + Classification Loss</p>
	<p>which then gets used to updated weights of YOLO architecture through backpropagation. YOLO architecture, however, have a few limitations: 1. <i>Prone to Localization Error</i> :</p>

	<h2><a id ="rt">Retina Net</a></h2>
	<p><b>Focal Loss:</b></p>
	<p>Focal Loss is designed to address the class imbalance problem, where the number of background (non-object) samples greatly outweighs the number of object samples</p>
	<p>Focal Loss is based on cross-entroy loss, but designed to down-weight easy (well-classified) examples, and focussing on hard negatives (misclassified) examples during training. 
	It adds a modulating factor `(1 - p_t)^&gamma;` to cross-entroy, and tunable parameter `&gamma; &gt;0`, and Focal Loss can be stated as: </p>
	<p>&emsp;&emsp;&emsp;`FL(p_t) = -(1 - p_t)^&gamma;log(p_t)` <br>
	where, `p_t` is the predicted probability of the true class label.</p>
	<p>Here, when an example is misclassified and `p_(t)` is small, the modulating factor is near `1` and almost equivalent to cross-entropy. On the other hand, if `p_(t)` is large i.e, `p_(t) -> 1`, 
	the modulating factor goes to `0`, and loss for well-classified example is down-weighted.<br>
	However, for practical purposes, we add a balancing factor `&alpha;_(t)` to Focal Loss, making it</p>
	<p>&emsp;&emsp;&emsp;`FL(p_t) = -&alpha;_(t)(1 - p_t)^&gamma;log(p_t)`</p>
	<p>`&alpha;_(t)` ,as a balancing factor, adjusts the weight of positive (presence of object) and negative (non-bject) examples in the loss. Specifically, `&alpha;` represents the weight assigned to the positive examples relative to the negative examples. 
	By default, `&alpha;` is set to 0.25, which means that positive examples <i>(anchorboxes with object)</i> have a weight of 0.25, while negative examples <i>(anchorboxes with non-objects)</i> have a weight of `1 - &alpha; = 0.75`.</p>
	<p><b>Retina Net Architecture :</b></p>
	<p>Architecture of Retina Net is composed of one <i>backbone network</i> that is responsible for computing a convolutional feature map over the entire image, and two <i>task-specific subnets</i>.
	The ﬁrst is <u>Classification Subnet</u> performing convolutional object classiﬁcation on the backbone’s output, and the second is <u>Box Regression Subnet</u> performs convolutional bounding box regression
	</p>
	<img src="assets/img/retina_net.png" alt="retina_net", width="780", height="320">
	

	<h2><a id="ssd">Single Shot Detector (SSD)</a></h2>
	<p></p>



<h3>References :</h3>
<ul>
	<li>[1] Redmon, J., Divvala, S., Girshick, R. and Farhadi, A., 2016. You only look once: Unified, real-time object detection. In <i>Proceedings of the IEEE conference on computer vision and pattern recognition </i>(pp. 779-788).</li>
	<li>[2] Lin, T.Y., Goyal, P., Girshick, R., He, K. and Dollár, P., 2017. Focal loss for dense object detection. In <i>Proceedings of the IEEE international conference on computer vision </i>(pp. 2980-2988).</li>
	<li>[3] Lin, T.Y., Dollár, P., Girshick, R., He, K., Hariharan, B. and Belongie, S., 2017. Feature pyramid networks for object detection. In <i>Proceedings of the IEEE conference on computer vision and pattern recognition</i> (pp. 2117-2125).</li>
	<li>[4] </li>
</ul>
</body>
</html>
