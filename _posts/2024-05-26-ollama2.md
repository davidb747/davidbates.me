---
layout: article
title: "Ollama로 Llama3 실행하기 (2/2): HuggingFace에서 한국어 Llama3 사용하기"
aside:
  toc: true
cover: /assets/ml/llama3_thumbnail_2.png
excerpt: HuggingFace와 Ollama를 이용한 Custom model serving 방법에 대해 알아보겠습니다.
---

  
지난 글에서 Ollama를 이용해서 Llama3를 사용하는 방법을 공유드렸었습니다.

<br>

저번에 사용한 모델은 Meta에서 발표한 Base Llama3 모델인데요.

해당 모델은 한국어에 튜닝되어있지 않다보니 우리가 사용하기엔 불편했죠. 

다행히도 여러 능력자분들께서 빠르게 한국어 Fine Tuning 모델을 만들어서 공유해주셨습니다.

이번 포스팅에서는 HuggingFace에 공유된 한국어 Fine Tuning된 Llama3 모델을 Ollama로 실행하는 법을 알려드릴게요.

<br>

오늘 포스팅도 테디노트님의 유튜브를 많이 참고했습니다. 

[해당 영상](https://youtu.be/VkcaigvTrug?si=yCTRsLCg3Q1pYQZy)은 꼭 보시는 걸 추천드립니다. 

<br>

<br>

# 1. HuggingFace에서 한국어 모델 다운 받기 

**HuggingFace는 각종 trained 모델을 공유하는 일종의 모델 허브**에요. 

오늘 사용할 한국어 Tuned Llama3도 여기에 공유되어 있죠. 

[이 링크](https://huggingface.co/heegyu/EEVE-Korean-Instruct-10.8B-v1.0-GGUF)로 들어가시면 야놀자에서 배포한 한국어 Llama3 모델을 다운로드 받을 수 있어요. 

'Files'에서 GGUF 파일을 다운로드 받으시면 되는데요. 

잠깐 GGUF 포맷에 대해 설명하고 갈게요.

GGUF는 딥러닝 모델을 저장하는 파일 포맷 중 하나에요.

**GGUF 포맷은 고용량의 모델 저장에 효율적이라 LLM 모델 저장에 주로 사용되고 있어요.** 

![eeve_page](/assets/ml/eeve.png)

위의 모델들 중 GGUF 파일 아무거나 하나를 다운로드 받으시면 돼요. 

저는 `ggml-model-Q5_K_M.gguf` 파일을 사용할게요. 

다운로드 받은 다음에 작업을 수행할 working directory로 이동시켜주세요. 

<br>

<br>

# 2. Modelfile 만들기

우리는 방금 HuggingFace에서 GGUF 모델을 다운로드 받았어요. 

**그럼 이 모델을 Ollama에서 실행할 수 있도록 해줘야겠죠.** 

**이를 위해서 필요한게 Modelfile이에요.** 

Modelfile은 gguf 파일과 동일한 위치에 생성해주시면 돼요. 

본 포스팅에서는 제가 사용한 Modelfile을 공유드릴게요. 

```
FROM ggml-model-Q5_K_M.gguf

TEMPLATE """{{- if .System }}
<s>{{ .System }}</s>
{{- end }}
<s>Human:
{{ .Prompt }}</s>
<s>Assistant:
"""

SYSTEM """A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."""

PARAMETER temperature 0
PARAMETER stop <s>
PARAMETER stop </s>
```

<br>

오늘 포스팅에 사용하는 `ggml-model-Q5_K_M.gguf`은 해당 Modelfile로 동작되니, 그대로 복사하셔서 사용하시면 됩니다. 

다른 모델을 사용하고자 한다면 Modelfile도 변경해주셔야하는데요. 

작성 방법은 [Ollama GitHub](https://github.com/ollama/ollama/blob/main/docs/modelfile.md)에 상세히 적혀있으니 직접 보시고 작성하시면 될 것 같아요. 

<br>

추후 이에 대한 내용도 포스팅으로 다뤄볼게요. 

<br>

<br>

# 3. Ollama용 모델로 변환 

이제 **모델 파일을 이용해서 GGUF 파일을 Ollama용 모델로 변환**합니다. 

GGUF 파일과 Modelfile이 위치한 폴더로 이동 후 아래의 커맨드를 입력하면 모델 파일로 변환 됩니다. 

```
ollama create <YOUR_MODEL_NAME> -f Modelfile

# 예시 
ollama create ollama_model -f Modelfile
```

<br>

```
# 생성된 모델 확인 
ollama list
```

<br>

모델 생성 후에는 ollama list를 통해 제대로 모델이 생성됐는지 확인해주세요. 

`<YOUR_MODEL_NAME>:latest` 라는 이름으로 모델이 생성되어 있어야합니다. 

<br>

<br>

# 4. 생성된 모델 실행하기 

이제는 생성된 모델을 실행해주기만 하면 됩니다. 

```
ollama run <YOUR_MODEL_NAME>

# 예시
ollama run ollama_model
```

<br>

<br>

이렇게 총 두 편에 걸쳐 Ollama를 이용해 로컬 서버를 구축하는 방법에 대해 설명드렸는데요. 

프로그래밍에 대한 기초적인 지식이 있으신 분들이라면 어렵지 않게 따라하실 수 있을 거에요. 

다만, 프로그래밍을 완전 모르는 초보자분들은 조금 어려울 수도 있을 것 같네요. 

그런 분들은 LM Studio라는 GUI 기반 툴이 있으니 해당 애플리케이션을 이용해서 서버를 구축해보시면 좋을 것 같아요. 

이에 관한 내용은 역시 [테디노트 유튜브 채널](https://youtu.be/bANQk--Maxs?si=fgvZCAK1_zeSJqMB)에서 확인해보시면 좋을 것 같아요. 

여력이 된다면 제 블로그에서도 포스팅으로 다뤄보도록 하겠습니다.

<br>

<br>