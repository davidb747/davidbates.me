---
title: Activation Functions, Optimization Methods, and Loss Functions
tags: northumbria-article
sidebar:
  nav: "docs-en"
---

## Activation Functions
### Sigmoid 
### TanH
### ReLU, LeakyReLU
### Exponentially Linear Units (ELU)
### Gated Linear UNits (GLU)
### Swish Activation
### Mish Activation

## Optimization Methods
### Gradient Descent
### Mini-Batch Gradient Descent
### ADAM
### RMSProp
### AdaGrad
### Adadelta
### Nesterov Acc. Gradient (NAG)
### L-BFGS

## Loss Functions