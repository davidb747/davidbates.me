---
layout: article
title: Self-Supervised Learning
date: 2023-05-13
sidebar:
  nav: "docs-en"
mathjax: true
---

Self-supervised learning is a deep learning method in which a model learns representation form unlabeled data. 
It leverages data's inherent co-occurence relationship or lack thereof as self-supervision for labels (Liu et al., 2021). Yann LeCun put it as, _"the machine predicts any parts of its input for any observed part"_

Self supervised learning can be summarized as:
- Obtain "labels" from data itself by using internal structure
- Predict part of the data from the other parts, where other parts could be incomplete, transformed, distorted or corrupted (Liu et al., 2021)

For ease of purposes, we can separate self-supervised learning into three categories:
- __Generative :__ In this, we have an encoder and a decoder. We encode an input $x$ into an explicit vector $z$ and using reconstruction loss, decoder afterwrads tries to reconstruct $x$ from $z$. Generative methods are <a href="autoen.html">AutoEncoder and its variants</a> 
- __<a href="sl.html#cl">Contrastive :</a>__ In contrastive learning, we have an encoder, and a discriminator. Here, we encode an input $x$ into an explicit vector $z$. 
    Discriminator then uses similarity metrics such as InfoNCE to maximize the similarity between positive samples, and minimize the similarity between negative samples.
- __Generative - Constrastive :__ Here, we have an encoder-decoder where $z$ implicitly modelled to generate fake samples, and then a discriminator using distribution divergence loss such JS-Divergence, Wasserstein Distance tries to distinguish fake samples from the real ones. <a href="gan.html">GAN and its variants</a> fall into this category.

## Contrastive Learning

There are many ways to approach contrastive learning problem.
One of them is __Mutual Information__.

Mutual Information (MI) measures the degree of dependence or correlation between the input data and the learned representation. The goal is to maximize the MI between input data and learned representation where positive pairs share more MI than negative pairs.

If $p(x)$ is the data distribution <br>
$p(x, c)$ is the joint distribution for data and representation<br>
$p(c)$ is the marginal distribution of the representations<br>
and $X$ and $C$ are the random variables associated with data and representations, respectively.

Then Mutual Information (MI) (Song and Ermon, 2020) can be given as:

<p>`I(X, C) &colone; E_((x, y)&sim;p(x,y)) [log \frac{p(x,c)}{p(x)p(c)}]`</p>

### Contrastive Predictive Coding

One of the approach that uses MI is Contrastive Predictive Coding (CPC).

In CPC, a non-linear encoder $g_{enc}$ maps the input data sequences $x_t$ to latent representations $z_t = g_{enc}(x_t)$. Then, an autoregressive model $g_{ar}$ summarizes all $z_{\le t}$ and produces a latent representation $y_t = g_{ar}(z \le t)$ (Oord _et al.,_ 2019) as shown in the image below:

![CPC](/assets/img/cpc.png)

Then, if that CPC is an $m$-class classification problem, wher the goal is to distinguish a positive pair $(x, c) &sim; p(x, c)$ from $(m -1)$ negative pair $(x, c̄ ) &sim; p(x)p(c)$

We optimize the loss for a batch of $n$ positive pairs ${(x_{i}, c_{i})}_{i=1}^n$ as:

<p>`L(g) &colone; E [\frac{1}{n} \sum_(i=1)^n log \frac{m &sdot; g(x_(i),c_(i))}{g(x_(i), c_(i)) + \sum_(j=1)^(m-1) g(x_(i), c_(i,j)̄ )}]`</p>

This loss function is called Information Noise Contrastive Estimation or __InfoNCE__ and used in many further proposed methods in Contrastive Learning.

Both encoder and autoregressive model jointly optimize the loss using InfoNCE, and CPC afterwards models a density ratio which preserves the Mutual Information between $x_{t+k}$ and $c_t$ (Oord _et al.,_ 2019) as follows :

<p>`f_(k)(x_(t+k),c_(t)) &prop; \frac{p(x_(t+k) | c_(t))}{p(x_(t+k))}`</p>

In CPC, both $z_t$ and $c_t$ can be used as representation for downstream task, when more information from past is necessary $c_t$ is suitable.

### Deep Info MAX
(Jaiswal et al., 2020)

### SimCLR

<p>SimCLR is one of the simplest yet effective method proposed for contrastive learning of visual representations. Key features of simCLR framework are :</p>
<ul>
  <li><b>Data Augmentation </b>: Using stochastic data augmentation, first two correlated data samples are generated from one, as `\tilde(x_i)` and `\tilde{x_j}`</li>
  <li><b>Encoder</b> : A neural network <i>base encoder</i> `f(&sdot;)` is used to map augmented data samples to a lower-dimensional representation. SimCLR used <a href="cnn_dp.html#rn">ResNet</a> (Chen et al., 2020) to obtain `h_i = f(\tilde{x_i}) =`ResNet `(\tilde{x_i})` where `h_i &isin; &Ropf;^d` is the ouput after the average pooling layer</li>
  <li><b>Projection Head</b> : Afterwards, a small neural network <i>projection head</i> `g(&sdot;)` is used with one hidden layer to obtain latent space `z_i` where, `z_i = g(h_i) = W^(2)&sigma;(W^(1)h_i)` where &sigma; is a ReLU function. It is beneficial to maximize agreement using contrastive loss on `z_i` than `h_i`.</li>
  <li><b>Contrastive Loss Function</b> : Given a set of `{\tilde{x_k}}` including positive pair of `\tilde{x_i}` and `\tilde{x_j}`, it aims to identify `\tilde{x_j}` in `{\tilde{x_k}}_k\nei`. SimCLR framework is illustrated in the image below :</li>
</ul>

![SimCLR](/assets/img/simclr.png)

#### SimCLR Optimization Process

For the optimization in SimCLR, we have a minibatch of $N$ data samples which gets augmented into $2N$ samples. Negative data example are not explicitly sampled, but given a positive pair, remaining $2(N-1)$ are treated as negative ones.

The loss function of positive pair of examples $(i,j)$ is defined as:

<p>`l_(i,j) = -log frac{exp(sim(z_i,z_j)// &tau;)}{\sum_(k=1)^(2N) 1_(k&ne;i) exp((z_i, z_k) // &tau;)}`</p>

where,<br>
$sim(\cdot, \cdot)$ is cosine similarity function i.e, $sim(u, v) = \frac{u^T v} {\lVert u \rVert \, \lVert v \rVert}$ to normalize the representation<br>
$\tau$ is a temperature parameter<br>
$1_{k ≠ i} \in \{0, 1\}$ is an indicator function if and only if $ k ≠ i$

The final loss is computed as : $\mathfrak{L} = \frac{1}{2N} \sum_{k=1}^{N} [l(2k-1, 2k) +l(2k, 2k-1)]$ and is called __NT-Xent__ (Normalized Temperature Scaled Cross Entropy Loss)

## References

- Liu, X., Zhang, F., Hou, Z., Mian, L., Wang, Z., Zhang, J. and Tang, J., 2021. Self-supervised learning: Generative or contrastive._IEEE transactions on knowledge and data engineering_, 35(1), pp.857-876.
- Jaiswal, A., Babu, A.R., Zadeh, M.Z., Banerjee, D. and Makedon, F., 2020. A survey on contrastive self-supervised learning. _Technologies_ 9(1), p.2.
- Oord, A.V.D., Li, Y. and Vinyals, O., 2018. Representation learning with contrastive predictive coding. _arXiv preprint arXiv:1807.03748._
- Song, J. and Ermon, S., 2020. Multi-label contrastive predictive coding. _Advances in Neural Information Processing Systems_, 33, pp.8161-8173.
- Chen, T., Kornblith, S., Norouzi, M. and Hinton, G., 2020, November. A simple framework for contrastive learning of visual representations. _In International conference on machine learning_ (pp. 1597-1607). PMLR.
