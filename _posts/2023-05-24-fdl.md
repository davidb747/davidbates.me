---
layout: article
title: Federated Learning
date: 2023-05-24
sidebar:
  nav: "docs-en"
---


<html>

<!--- Adding Google Analytics -->
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-154990580-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-154990580-2');
</script>
<!-- End of Google Analytics Code -->
<!-- Adding MathJAX -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
  <script async="true" src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=AM_CHTML"> </script>
<!-- End of MathJAX -->


<body>
<p>
Federated Learning (FL) is a new machine learning technique where there is no need to collect raw data at a single data center before training from each device. 
In FL, training is performed distributedly on each mobile device separately, and locally computed updates on the current global model are then shared. 
This helps protect users' privacy as only updates (and not the data) are shared for a short time, and information on the source of updates is not needed for the optimization process in FL.
</p>
<p>
FL, as takes updates from each mobile, few things differ from traditional machine learning and distributed computing.<br>
<ul>
  <li><b>Non-IID Data :</b> Devices used for training data can vary in hardware capibilities, sensors, operating systems. Similary, user can vary based on geograhic and demographic, and consequently their usage pattern and preferences. Therfore, any user's local dataset cannot be representative of population distribution.</li>
  <li><b>Unbalanced Data :</b> Even two similar users can varying amount of training data based on heavy or light usages.</li>
  <li><b>Massively Distributed :</b> Data can be distributed among tens of thousand of users, where average number of data generated per user can be less than total number of users.</li>
  <li><b>Limited Communication :</b> Devices can be frequently switched off or on restricted/poor internet connection. Therefore, each client tends to participate for only a small number of update round each day.</li>
  <li><b>Communication Cost :</b> Unlike distributed systems, computation cost at data center is essential free in FL, but communication cost of for each round of updates is manifold.</li>
</ul>
</p>
<h4>Federated Learning Optimization :</h4>
<p>In FL, we assume a iterative process of synchronous updates in each round of communication.<br>
If there is a fixed set of `K` clients, each with a fixed local dataset. At the begining of each round, a random fraction of `C` clients is selected, and server sends the current global algorithm state to each of `C` clients. 
Each selected client then performs local computation based on global state and its own dataset, and sends an update back to server. The server then applies these updates to its global state, and the process repeats.</p>
<p>FL objective function can be stated as :</p>
<p>`min_{w&isin;&Ropf;^d} f(w)` &emsp;&emsp; where `f(w) &esdot; \frac{1}{n} \sum_{i=1}^n f_(i)(w)` &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;eq(1)</p>
<p>Here, as it is inherently a machine learning problem, `f_(i)(w) = l(x_(i), y_(i);w)` i.e, loss of prediction on data `(x_(i),y_(i))` with model parameter `w`</p>
<p>Also, as we have `K` clients over which data is partitioned as `P_(k)` and set of indexes of data points on each client `k` as `n_(k) = |P_(k)|`, then we can re-write eq(1) as: </p>
<p>&emsp; `f_(w) = \sum_{k=1}^K \frac{n_(k)}{n} F_(k) (w)` &emsp;&emsp;&emsp;`F(w) = \frac{1}{n_(k)}\sum_(i&isin;P_(k)) f_(i)(w)`</p>

<h4>FederatedAveraging Algorithm</h4>
<p></p>
<h3>References :</h3>
[1] McMahan, B., Moore, E., Ramage, D., Hampson, S. and y Arcas, B.A., 2017, April. Communication-efficient learning of deep networks from decentralized data. In <i>Artificial intelligence and statistics</i> (pp. 1273-1282). PMLR.<br>

</body>
</html>
