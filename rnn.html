---
layout: article
sidebar:
  nav: "docs-en"
---
<!DOCTYPE HTML>
<html>

<head>
<title>Recurrent Neural Network - Git Page - Konark Karna</title>
<meta charset="utf-8"/>
<meta name="author" content="Konark Karna">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="keywords" content="Recurrent Neural Network RNN Vanishing Gradient Exploding Gradient Konark Karna, Konark Karna India, Konark Karna Northumbria University, Konark Karna Newcastle, Konark Karna Computer Science,Data Scientist, Machine Learning Engineer, MSc Advanced Computer Science, Northumbria University,
			       Data Analysis, Data Visualization, Machine Learning, Neural Networks, Deep Learning, Natural Language Processing">
<link rel="stylesheet" type="text/css" href="basic.css">

<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href='http://fonts.googleapis.com/css?family=Play' rel='stylesheet' type='text/css'>
<link href='http://fonts.googleapis.com/css?family=Exo+2:400' rel='stylesheet' type='text/css'>
<link href='http://fonts.googleapis.com/css?family=PT+Sans+Narrow' rel='stylesheet' type='text/css'>
<link href="https://fonts.googleapis.com/css2?family=Cinzel:wght@500&family=Josefin+Sans&family=Balthazar&family=Ropa+Sans&display=swap" rel="stylesheet"> 
</head>
<!--- Adding Google Analytics -->
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-154990580-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-154990580-2');
</script>
<!-- End of Google Analytics Code -->
<!-- Adding MathJAX -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
  <script async="true" src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=AM_CHTML"> </script>
<!-- End of MathJAX -->


<body>

	<section id="text">
		<p class="heading">Recurrent Neural Networks</p>
        <p class = "dateline"><a href="notes.html"> &lt;&lt; Notes</a> || Date: 25<sup>th</sup> Apr 2022</p>
		<section id = "page">
		<a href="rnn.html#ar">1. RNN Architecture</a><br>
		<a href="rnn.html#df">2. Difficulty in Training RNNs</a>
            <p>Recurrent Neural Network is a type of neural network that is most suitable with sequential data or time series data.
                It is heavily relied upon in the cases of ordinal or temporal problems, e.g., language translation, speech recognition, image captioning, etc.</p>
            <h1><a id="ar">RNN Architecture</a></h1>
				<p>Architecture of RNNs makes it suitable to solve those problems. In RNNs, firstly, hidden layers are grouped together to form a node. 
                Afterwards, to predict an output value, each node at its time step, not only take input at that time step, but also from the node at previous time step.</p>
                <img src="images/RNN arch.png" alt="RNN Architecture" width="520" height="250" class="image_full">
            <p>From the left side of image, we can understand that, both <b>H1</b>, and <b>H2</b> hidden unit of neural network gets grouped together and hidden under node <b><i>s</i></b> in RNN with value <b><i>s<sub>t</sub></i></b> at time <b><i>t</i></b> (LeCun, Bengio and Hinton, 2015). <br>
				Also, we know that, each node in RNNs gets output from other neurons at the previous time step. It can be observed in the right side of the image that any output  <b><i>o<sub>(t+1)</sub></i></b>  at any time step <b><i>t+1</i></b>  is influenced by all input values <b><i>x<sub>t</sub> , x<sub>(t-1)</sub> .. x<sub>(t-n)</sub></i></b> from previous time steps. 
                <br>However, it is not enforced to follow the architecture on the right side, rigorously. 
                RNNs architecture are can be of many types. Few are illustrated below in images:</p>
				<img src="images/rnn.png" alt="RNN Architecture" width="600" height="400" class="image_full"><br>
			<p>
				With these varied type of architecture RNNs get to learn long-term dependencies and becoming one of the efficient neural network and based on the problem at hand, one of RNNs architecture illustrated in images above can be used. 
				For instance, in the case of music-generation problem, one-to-many architecture could be used. 
				On the other hand, in case of sentiment analysis, many-to-one RNN architecture would be suitable. <br>
				In general, RNNs architecture can be mathematical expressed as below: <br> </p>
			<p>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`X_(t) = W_(rec) &sigma; (X_(t - 1)) + W_(In) * u_(t) + b`&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<b>...(1)</b>
			</p>
			<p>
				In the equation, `W_(rec)` is recurrent weight, `&sigma;` is element-wise function, `W_(In)` is input weight collected in `&theta;`, `u_(t)` is input, `b` is bias, and `X_(t)` is state at time `t` (Pascanu, Mikolov, and Bengio, 2013).
			</p>
			<h1><a id="df">Difficulty in Training RNNs</a></h1>
			<p>
				When Backpropagation Through Time (BPTT) was proposed, it was RNNs that got the best of it. However, while backpropagating through deep RNNs, exploding or vanishing gradients can hinder learning process as the same weight i.e.,  `W_(rec)` is being shared by all the nodes in the architecture.<br>
				Let's try to understand it well - In the above eq(1) of RNN architecture, we can understand that the cost function,   `&epsilon; = \sum_{1\let\leT} &epsilon;X_(t)` gets to measure the performance of some task, where `&epsilon;X_(t) = &tau;(x_(t))` and therefore gradients for backpropagation can be expressed with: </p>
			<p>
				&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`\frac{&part;&epsilon;}{&part;&theta;} =\sum_{1\let\leT} \frac{&part;&epsilon;_(t)}{&part;&theta;}` &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<b>...(2)</b><br>
				&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`\frac{&part;&epsilon;_(t)}{&part;&theta;}=\sum_{1\lek\leT}(\frac{&part;&epsilon;_(t)}{&part;X_(t)}* \frac{&part;X_(t)}{&part;X_(k)}* \frac{&part;^+X_(k)}{&part;&theta;})` &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;&nbsp;<b>...(3)</b> <br>
				&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`\frac{&part;X_(t)}{&part;X_(k)}= \Pi_(t\gei\gtk)\frac{&theta;X_(i)}{&theta;X_(i-1)}= \Pi_{t\gei\gtk}&nbsp; \W_{rec}^T &nbsp;diag(&sigma;^'(X_(i-1)))`&emsp;&emsp;&emsp;&emsp;<b>...(4)</b><br>
			</p>
			<p>
				In eq(3) the gradient component `\frac{&part;&epsilon;_(t)}{&part;&theta;}` is a sum of temporal contribution at given node. We can see that temporal contribution `\frac{&part;&epsilon;_(t)}{&part;X_(t)}* \frac{&part;X_(t)}{&part;X_(k)}* \frac{&part;^+X_(k)}{&part;&theta;}`measures how `&theta;` at step `k` affects the cost at step `t > k`.
				This gradient component `\frac{&part;&epsilon;_(t)}{&part;&theta;}` transport the error <i>in time</i> from step `t` back to step `k`. Next, we need to observe in the RHS of eq(4), here `\W_{rec}^T` is constant through all the time steps, and gets multplied with `diag` that converts vector into diagonal matrix, and `&sigma;^'` computing element-wise the derivative of `&sigma;`. 
			</p>
			<p>
				Therefore, <br>
				If  `W_(rec)` is small, gradient starts to get smaller n smaller when pushed back into network, and neurons at earlier time steps fails to learn anything. This phenomenon is called <b>Vanishing Gradient Problem</b>.<br>
				If  `W_(rec)` is large, backpropagating gradients start to grow exponentially. This is <b>Exploding Gradient Problem</b>
			</p>
			<p>
				Exploding gradient can be solved in two ways i.e., gradient scaling or gradient clipping. <br>
				In <i>Gradient Scaling</i>, we normalize/re-scale the error gradient vector norm if the vector norm crosses a certain threshold value, a value defined by us. <br>
				On the other hand, in <i>Gradient Clipping</i>, if vector norm crosses the pre-defined threshold value, we force gradient values to a specific minimum or maximum value.
			</p>
			<p>
				Vanishing gradient can be solved by slightly tweaked architecture of <a href="lstm.html">LSTM and GRU</a>.
			</p>
			<p>
				<b>
					References: 
				</b> <br>
				[1] Pascanu, R., Mikolov, T. and Bengio, Y., 2013, February. On the difficulty of training recurrent neural networks. In <i>International conference on machine learning </i>pp. 1310-1318. <br>
				[2] LeCun, Y., Bengio, Y. and Hinton, G., 2015. Deep learning. <i>Nature</i>, 521(7553), pp.436-444. <br>
			</p>
		</section>

	</section>
</body>
</html>
