---
layout: article
title: Feature Detection and Matching
sidebar:
  nav: "docs-en"
---
<!DOCTYPE HTML>
<html>


<!--- Adding Google Analytics -->
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-154990580-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-154990580-2');
</script>
<!-- End of Google Analytics Code -->
<!-- Adding MathJAX -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
  <script async="true" src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=AM_CHTML"> </script>
<!-- End of MathJAX -->


<body>
        <p>Date: 13<sup>th</sup> May 2022</p>
		
		<p> Keypoint Feature Detection and Matching can be divided into four stages:<br>
		<a href="fd.html#fd">Feature Detection</a> where each image is searched for locations that are likely to match in other images.<br>
		Then, <a href="fd.html#fr">Feature Description</a> where each region around detection keypoint locations is converted into a more compact and stable (invariant) <i>descriptor</i> that can be matched against other descriptors.<br>
		<a href="fd.html#fm">Feature Matching</a>, afterwards, efficiently searches for likely matching candidates in other images.<br>
		Lastly, <a href="fd.html#ft">Feature Tracking</a>. It only seaches a small neighbourhood around each detected feature. Sutiable for video processing.
		</p>
		<h3><a id="fd">Feature Detectors</a></h3>
		<p>One of the simplest matching creteria for two image patches is their <u>weighted squared summed difference, WSSD</u> :</p>
		<p>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`E_(WSSD)(u) = \sum_{i}w(x_(i))[I_(1)(x_(i) + u) - I_(0)(x_(i))]^2`</p>
		<p>where `I_(0)` and `I_(1)` are the two images being compared, `u = (u,v)` is the <i>displacement vector</i>, w(x) is a spatially varying weighting function, and the summation `i` is over all the pixels in the patch. <br>
		With this method, as we do not know which other image locations the feature will end up being matched up against, it is suitable to compare it small variation in position `&Delta;u` by comparing an image patch against itself, this is known as <i>auto-correlation function</i> or <i>surface</i></p>
		<p>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`E_(AC)(&Delta;u) =  \sum_{i}w(x_(i))[I_(0)(x_(i) + &Delta;u) - I_(0)(x_(i))]^2` &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;...eq(1)</p>
		<p>Using a Taylor-Series expansion of the image function, `I_(0)(x_(i) + &Delta;u) &asymp; I_(0)(x_(i)) + &nabla; I_(0)(x_(i)) &sdot;&Delta;u`, the auto-correlation function from eq(1) can be approximated as :</p>
		<p>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`E_(AC)(&Delta;u) =  \sum_{i}w(x_(i))[I_(0)(x_(i) + &Delta;u) - I_(0)(x_(i))]^2`<br>
		&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;&nbsp;&nbsp;`&asymp; \sum_{i}w(x_(i))[I_(0)(x_(i)) + &nabla;I_(0)(x_(i)) &sdot; &Delta;u - I_(0)(x_(i))]^2`<br>
		&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;&nbsp;&nbsp;`= \sum_{i}w(x_(i))[&nabla;I_(0)(x_(i))&sdot; &Delta;u]^2`&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; .... eq(2)<br>
		&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;&nbsp;&nbsp;`=&Delta;u^T A &Delta;u`&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;&nbsp;&emsp;...eq(3)</p>
		where,<br>
		&emsp;&emsp;&emsp;`&nabla; I_(0)(x_(i)) = (\frac{&part;I_(0)}{&part;x},\frac{&part;I_(0)}{&part;x})(x_(i))` is the <i>image gradient</i> at `x_(i)`. <br>
		This gradient can be computed using a variery of techniques. The classic "Harris" detector uses a [-2 -1 0 1 2] filter, but modern variants convolve the image with horizontal and vertical derivatives of a Guassian (typically with `&sigma;=1`).
		The <i>auto correlation</i> matrix `A` in eq(3)  thus can be written as : <br>
		&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`A = w&lowast; [{:(\ \ \ \ I_x^2),(\ \ \ \ I_x I_y):} {:(\ \ \ \ I_x I_y),(\ \ \ \ I_x^2):}]`&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; ...eq(4)<br>
		here, we replaced weighted summation with discrete convolution in eq(2) with weighting kernel `w` which when convolved with the outer product of the gradients `&nabla;I` provides us a per-pixel estimate of the local shape of auto-correlation function.
		<p><b><u>Forstner-Harris :</u></b></p>
		<p>The inverse of matrix `A` provides a lower bound on the uncertainty in the location of a matching patch. It is, therfore, a useful indicator of which patches can be reliable matched. The eigenvalue analysis of auto-correlation matrix `A` that provide two eigenvalues (`&lambda;_0,&lambda;_1`) and two eigenvector directions is a way to evalute this uncertainty as larger uncertainty depends on the smaller eigenvalue i.e, `&lambda;_0^(-1/2)`
		Forstner-Harris, however, proposed a simpler way: </p>
		<p>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;det`(A) - &alpha;&nbsp;` trace `(A)^2 = &lambda;_0&lambda;_1 - &alpha;(&lambda;_0 + &lambda;_1)^2` &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; ...eq(5)</p>
		<p>with `&alpha; = 0.06`. This method uses Gaussian weighting window, it helps detector response insensitive to in-place image rotations, and downweighting edge-like features where `&lambda;_1 &Gt; &lambda;_0`</p>
		<p>>> Triggs (2004) suggested using the quantity : `&lambda;_0 - &alpha;&nbsp;&lambda;_1` with `&alpha; = 0.05` &nbsp;in eq(5) to reduce the response at 1D edges where aliasing error sometimes inflate the smaller eigenvalue</p>
		<p>>> Brown, Szeliski, and Winder (2005), on the other hand, suggested to use the harmonic mean : `\frac {det A}{tr A} =  \frac {&lambda;_0&lambda;_1}{&lambda;_0 + &lambda;_1}` which is a smoother function in the region where `&lambda;_0 &asymp; &lambda;_1`</p>
		<p><b><u>Adaptive non-maximal suppression (ANMS) :</u></b></p>
		<p>This method only detect features that are both local maxima and whose response value is significantly (10%) greater than all of its neighbours within a radius `r`.
		This helps to get an efficient way to associate suppression radii with all local maixma by first sorting them by their response strength and then sprting by decreasing suppression.<br>
		<img src="assets/img/anms.png" width="580" height="380" alt="anms" class="image_full">
		</p>
		<p><b><u>Scale Invariance</u></b></p>
		<p>In many situations, fine-scale features may not exist while matching images with little high-frequency detail. Therfore, extracting features at just different scales remain ineffective, making it essential to extract features that are stable in both <u>scale and space</u>.</p>
		<p>In such cases, Multi-scale representation `g_(G)(x,y,&sigma;)` is realised by convolving the image `I(x,y)` with Gaussian kernel `h_(G)(x,y,&sigma;)` with increasing `&sigma;` </p>
		<p>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`g_(G)(x,y,&sigma;) = h_(G)(x,y,&sigma;) &lowast; I(x,y)` &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;....eq(6)</p>
		<p>This is <u>scale-space function</u> where `&sigma;` is <u>scale parameter</u> and only kernel function to get above eq(6) is Gaussian :</p>
		<p>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`h_(G)(x,y,&sigma;) = \frac{1}{2 &pi; &sigma;^2} e^((-x^2+y^2)/(2&sigma;^2))`</p>
		<p>Of many proposal for this scale Invariance, first is to use extrema <i>(maximum and minimum values of fuction `g` )</i> in Laplacian of Gaussian(LoG) function as interest point locations. LoG kernel function written in normalized form is as : </p>
		<p style="font-size: 18px;">&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`h_(LoG)(x,y,&sigma;) = &sigma;^2&nabla;^2h_(G)(x,y,&sigma;) = &sigma;^2[h_(G_(xx))(x,y,&sigma;) + h_(G_(xy))(x,y,&sigma;)]`</p>
		<img src="assets/img/log.png" width="600" height="250" alt="log" class="image_full">
		<p>Another is to compute a Difference of Gaussian(DoG) - The DoG is obtained by performing the subtraction of two Gaussian kernels
		where a kernel must have a standard deviation slightly lower than the previous one and written as :</p>
		<p style="font-size: 18px;">&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`h_(DoG)(x,y,&sigma;) = h_(G)(x,y,k&sigma;) - h_(G)(x,y,&sigma;)` <br>
			&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`&asymp;(k-1) &sigma;^2&nabla;^2 h_(G)`</p>
		
		<p><b><u>Rotational Invariance and Dominant Orientation </u></b></p>
		<p>In addition to scale changes, another key thing is to deal with is - <i>in-place image rotation.</i> Rotational invariant descriptors can do that, but they have poor discriminability i.e, they map different looking patches to the same descriptors</p>
		<p>
		Better method is - <u>Dominant Orientation</u><br>
		In this, first step is to assign robust dominant direction of the points of interest, and to do this we consider `&sigma;` scale of any point of interest to select the Gaussian image `g_(G)(x,y,&sigma;)` with scale closest to the point in examination - making it scale invariant<br>
		Then, For the point considered g_(G)(x,y) we consider points around it (small window), and using selected scale `&sigma;` we calculate <u>gradient magnitude</u> `m(x,y)` and <u>orientation</u>`&theta;(x,y)`:
		</p>
		<p>
		&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`m(x,y) = \sqrt([g_(G)(x+1,y) - g_(G)(x-1,y)]^2+[g_(G)(x,y+1) - g_(G)(x,y-1)]^2)`</p>
		<p>
		&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`&theta;(x,y) = tan^-1[g_(G)(x+1,y) - g_(G)(x-1,y)]//[g_(G)(x,y+1) -g_(G)(x,y-1)]`
		</p>
		<p>With these information, a histogram of the local orientations to the selected scale is generated with 36 cells (bins) as angular resolution of the orientation histogram is 10&deg;<br>
		Each sample added to the histogram, then, weighted by the gradient and a Guassian function with a `&sigma;` that is 1.5 times larger that that of the scale of the point of interest with respect to circular window that is centered around.
		Afterwards, <u>dominant orientation</u> gets assigned to the peak value of the accumulated histogram. <br>
		<img src="assets/img/dom.png" width="600" height="170" alt="dominant orientation" class="image_full"><br>
		If there were more dominant peaks in histogram that exceeded 80% of the absolute maximum, then a parabolic interpolation is performed
		between the peak value and the values of the three adjacent points in the histogram.
		</p>

		<h3><a id="fr">Feature Descriptors</a></h3>
		<p>Feature Descriptor is to match detected keypoint features i.e, to determine which feature come from corresponding location in different images as even after compensating for local scale, orientation and affine deformation, the local appearance of image patches may still vary from image to image.
		Few proposed descriptors are : 
		</p>
		<p><b><u>Bias and gain Normalization (MOPS)</u></b></p>
		<p>Simplest is to just simply normalize intensity patches. But to compensate for slight inaccuraries in feature point detector, 
		Multi-scale oriented Patches (MOPS) are sampled at a spacing of five pixels relative to the detection scale, and then to compensate for affine photometric variations (linear exposure changes or bias or gain) 
		patch intensity are re-scaled so that their mean is zero and their variance is one. </p>
		<p><b><u>Scale invariant Feature Transform (SIFT)</u></b></p>
		<p>SIFT features are formed by computing the gradients at each pixel in a `16&times;16` window around the <i>detected keypoint.</i><br>
		In SIFT, then the coordinate of descriptor and orientations of local gradient of that `16&times;16` window are rotated with respect to the dominant direction of <i>detected keypoint</i> to obtain invariance. <br>
		Then, amplitude of the gradient of each sample of `16&times;16` window is weighted with Gaussian function with `&sigma;` of value equal to half of the scale of the keypoint to give greater weight to the samples closer to the position of <i>keypoint</i>. Then, `16&times;16` window is partitioned into `4&times;4` from which the orientation histograms are calculated as shown below :</p>
		<img src="assets/img/SIFT.png" width="530" height="220" alt="sift" class="image_full">
		<p>This orientation histogram has 8 bins each with resolution of 45&deg;. To improve accuracy of these local histogram, a trilinear interpolation is performed, where the value of each bin is multiplied by an additional weight coefficient of `1-d`, where `d` is the distance between the sample and the central position of the bin (in width units of the bins).
		</p>
		<p>The `4&times;4` sub-window with 8 histogram bins each then provides `4&times;4&times;8 = 128` elements <u> SIFT Descriptor vector.</u>
		</p>
		<p><b><u>PCA-SIFT</u></b></p>
		<p>It is inspired by SIFT, and computes `x` and `y` gradient derivatives over a `39&times;39` patch and then reduces the resulting 3042-dimensional vector to 36 using Principal Component Analysis (PCA) </p>
		<p><b><u>RootSIFT</u></b></p>
		<p>RootSIFT, again is inspired by SIFT, it simply re-normalize SIFT descriptor using an `L_(1)` measure and then taking the square root of each component. It is proved to dramatic improve performance.</p>
		<p><b><u>Gradient Location and Orientation Histograms (GLOH) Descriptor</u></b></p>
		<p>GLOH is a variant of SIFT, but instead of dividing local region into sub-square, it divides them into circular region. It is organised in a grid iin logarithmic-polar locations with a radial resolution of 3 bins (with a radius of 6,11, and 15) and an angular resolution of 8 intervals, resulting partition into `2&times;8 + 1 = 17` spatial bins. Each spatial bins then resolves 16 different orientations of the gradient. The descriptor, therefore, develops a total of `16&times;17 = 272` elements for each point of interest.<br>
		<img src="assets/img/gloh.png" width="400" height="220" alt="gloh" class="image_full"><br>
		GLOH, however, can be reduced down to 128 as in SIFT by applying PCA to descriptor vector.
		</p>


		<p><b><u>Speed Up Robust Feature (SURF)</u></b></p>
		<p> SURF descriptor is calculated in two steps:<br>
		1. It uses the <u>Haar Transform</u> to calculate the dominant direction of each point of interest.<br>
		2. a descriptor vector of 64 elements is generated which then describes the local information for each point of interest.</p>
		<p><u>Dominant Direction :</u> In this, for each point of interest, first we determine a circular window of radius `6 &sdot; s` where `s` is the scale on which the point of interest was determined.
		Then, the Haar transform is calculated in the direction of `x` and `y` for each point of this circular window using Haar box filters of size `2 &sdot; s`.<br>
		<img src="assets/img/surf1.png" width="600" height="180" alt="surf1" class="image_full"><br>
		The Dominant Direction is, then, determined through a conical window with an opening of `&pi;//3` that weighs with a Gaussian function of `2&sigma;` Haar's responsed included in this conical window  that slides around the point of interest. The dominant direction is then the resulting one with a greater total weight <i>( shown as purple in the image above )</i>. 
		</p>
		<p><u>Descriptor Vector :</u> It is constructerd by initially considering a square window of size `20&sdot;s` centered at the point of interest and oriented in the dominant direction <i>( calculated above )</i> Later, it is divided into 16 subsquare regions and for each of these, the wavelet transform of size `2&sdot;s` of Haar is applied for `5&times;5` spaced points. 
		Consider `d_(x)` and `d_(y)` as the responses of the transform along the respective axes `x` and `y` which are further weighted with Gaussian function (with `&sigma; = 0.33`) to improve the accuracy of localization. Afterwards, for each of the 16 subregions, the horizontal and vertical wavelet response and their absolute values are summed up producing a descriptor vector of 4 elements : </p>
		<p>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`v = { \sumd_(x), \sum|d_(x)|, \sumd_(y), \sum|d_(y)|}`</p>
		<p>making SURF descriptor vector of `4&times;16 = 64 ` elements. Compared to SIFT, SURF descriptor is more robust to noise as it does not consider the direction of the gradient individually. Also, there's version of SURF, without calculating the dominant direction called U-SURF.
		</p>




		<h3><a id="fm">Feature Matching</a></h3>
		<p>There are well-known evaluation metrics such as Precision, Recall, ROC Curve, mAP etc, that can be used. However, the most suitable for feature matching is <u>nearest neighbour distance ratio (NNDR)</u>
		</p>
		<p>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;NNDR` = \frac{d1}{d2} = \frac{||D_(A) - D_(B)||}{||D_(A) - D_(C)||}` <br>
		where `d_(1)` and `d_(2)` are the nearest and second nearest neighbour distances, `D_(A)` is the target descriptor, and `D_(B)` and `D_(C)` are its closest two neighbours</p>
		<h1><a id = "ft">Feature Tracking</a></h1>
		
		<p class="dateline">Source : <a href="https://link.springer.com/book/10.1007/978-3-030-42374-2" target="_blank">Handbook of Image Processing and Computer Vision by Arcangelo Distante, Cosimo Distante </a> and <a href="https://link.springer.com/book/10.1007/978-3-030-34372-9" target="_blank">Computer Vision by Richard Szeliski</a> </p>



</body>
</html>
