---
layout: article
sidebar:
  nav: "docs-en"
---

<!DOCTYPE HTML>
<html>

<!--- Adding Google Analytics -->
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-154990580-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-154990580-2');
</script>
<!-- End of Google Analytics Code -->
<!-- Adding MathJAX -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
  <script async="true" src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=AM_CHTML"> </script>
<!-- End of MathJAX -->


<body>
	
	<section id="text">
		<p class="heading">Temporal-Difference (TD) Learning</p>
        <p class = "dateline"><a href="notes.html"> &lt;&lt; Notes</a> || Date: 27<sup>th</sup> Apr 2022</p>
		<section id = "page">
            <p>
                Temporal-Difference Learning is a combination of Monte Carlo (MC) ideas and Dynamic Programming (DP) ideas. <br>
                MC method wait until the return following the visit is known, and then uses that return as a target for `V(S_(t))`. A single every-visit MC method suitable for non-stationary environments is: </p>
                <p>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`V(S_(t)) &larr; V(S_(t)) + &alpha;&#91;G_(t)-V(S_(t))&#93;`<br>
            where,<br> `G_(t)` is actual return following time `t` <br> `&alpha;` is a constant step-size parameter
            </p>
            <p><b>Temporal-Difference (TD) method </b>, unlike MC, doesn't need to wait until the end of episode, it only needs to wit till the next time step. Simplest TD: <br>
                &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`V(S_(t)) &larr; V(S_(t)) + &alpha;&#91;R_(t+1) + &gamma;V(S_(t+1))-V(S_(t))&#93;`&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; ...(1)<br>
                In effect, <br>
                the target of the MC method is `G_(t)`<br>
                the target of the TD update is `R_(t+1)+ &gamma;V(S_(t+1))`<br>
                Also, above mentioned simplest TD method is called <i>TD(0)</i> or <i>one-step TD</i>
            </p>
            <p>
                >> Because TD(0) bases its update in part on an existing estimate, we can say it is a <b>bootstrapping method</b> like DP.<br>
                We know<br></p>
            <p class="code">
                &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`v_(&pi;)= `&Eopf;<sub>`pi`</sub>`&#91;G_(t) | S_(t) = s&#93;`&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;...(2)<br>
                &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`=`&Eopf;<sub>`pi`</sub>`&#91;R_(t+1) +&gamma;G_(t+1) | S_(t)=s&#93;`<br>
                &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`=`&Eopf;<sub>`pi`</sub>`&#91;R_(t+1) +&gamma;v_(&pi;)(S_(t+1) | S_(t) =s)` &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;....(3)</p>
            <p>
                MC method uses estimate of eq(2) as target<br>
                DP method uses estimate of eq(3) as target<br>
                <b>TD method</b> combine the sampling of MC with bootstrapping of DP
                
            </p>
            <p>Quantity in TD(0) update in eq(1) is a sort of error. measuring difference between estimted value of `S_(t)` and the better estimate `R_(t+1) +&gamma;V(S_(t+1))`. 
            This is called <b>TD error</b></p>
            <p>
                &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`&delta;_(t)&esdot;R_(t+1) +&gamma;V(S_(t+1))-V(S_(t))`
            </p>
            <p><b>Note: </b>TD error is the error in the estimate made at that time, because it depends on the next state and next reward <i>(not available until next time step)</i> i.e., `&delta;` is error in `V(S_(t))` available at `t+1`<br>
            Also, if `V` doesn't change during episode (as in MC methods) then, MC error can be written as sum of TD errors.</p>

            <p style="font-size: 18px;">
                &emsp;&emsp;&emsp;&emsp;`G_(t)-V(S_(t))=R_(t+1) + &gamma;G_(t+1) - V(S_(t)) + &gamma;V(S_(t+1)) -&gamma;V(S_(t+1))`<br>
                &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;&nbsp;`=&delta;_(t) +&gamma;(G_(t+1) -V(S_(t+1)))`<br>
                &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;&nbsp;`=&delta;_(t) +&gamma;&delta;_(t+1) + &gamma;^2(G_(t+2)-V(S_(t+2)))`<br>
                &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;&nbsp;`=&delta;_(t) +&gamma;&delta;_(t+1) +&gamma;^2&delta;_(t+2) + ... + &gamma;^(T-t-1)&delta;_(T-1) + &gamma;^(T-t)(G_(T)-V(S_(T)))`<br>
                &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;&nbsp;`=&delta;_(t) +&gamma;&delta;_(t+1) +&gamma;^2&delta;_(t+2) + ... + &gamma;^(T-t-1)&delta;_(T-1) + &gamma;^(T-t)(0-0)`<br>
                &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;&nbsp;`=\sum_{k=t}^(T-1)&gamma;^(k-1)&delta;_(k)`<br>
            </p>
            If step-size is small, then it may still hold approximatly, even if `V `is updated during episode.

            <p><b>Advantanges of TD Prediction Methods :</b></p>
            <p>
                1. Over DP, TD methods do not require a model of the environments, of its reward and next-state probability distribution<br>
                2. Over MC, TD methods, in an online fully incremental fashion, only need to wait one time step whereas in MC one need to wait until the end of the episode<br>
                3. TD method learning from guess can guarantee convergence.<br>
                For any policy &pi;, TD(0) has been proved to converge to `v_(&pi;)` in the mean for a constant step size parameter if it is small ot with probability 1 if the step size parameter decreases according to the usual stochastic approximation conditions.
            </p>
            <p>
                <b>Optimality of TD(0) :</b>
            </p>
            <p>
                In case of limited experience, we increment learning methods to present the exprience repeatedly until the method converges upon an answer.<br>
                Aftetr approximating a value function, `V`, all the available exprience is processed again with the new value function to produce a new overall increment and so on, until the value function converges. This is called <b>batch updating</b><br>
                Under batch updating, TD(0) converges deterministically to a single answer independent of the step size parameter, `&alpha;` as long as `&alpha;` is small.
            </p>

            
		</section>

	</section>

	<footer>
		<p> Follow On: </p>
		<a href="https://www.linkedin.com/in/k-karna/"><img src="images/in.png" alt="linkedin" width="20" height="20"></a>
		<a href= "https://www.github.com/k-karna"><img src="images/gt.png" alt="github" width="20" height="20"> </a>
        <a href= "https://www.twitter.com/konarkkarna"><img src="images/twitter.png" alt="twitter" width="20" height="20"></a>
		<p>Last Updated: Apr 27, 2022<br>
	  </footer>
</body>
</html>
