---
layout: article
title: Policies and Value Function
sidebar:
  nav: "docs-en"
---
<!DOCTYPE HTML>
<html>
<!--- Adding Google Analytics -->
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-154990580-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-154990580-2');
</script>
<!-- End of Google Analytics Code -->
<!-- Adding MathJAX -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
  <script async="true" src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=AM_CHTML"> </script>
<!-- End of MathJAX -->


<body>
        <p>Date: 30<sup>th</sup> Apr 2022</p>

        <p><b>Value Function :</b> Function of states (or of state-action pair) that estimate how good it is for the agent to be in the state. <i>('how good' is in terms of future rewards or expected return)</i></p>
        <p><b>Policy:</b> It is mapping from states to probabilities of selecting each possible actions. If agent is following policy `pi` at time `t` then `pi(a |s)` is the probability that `A_(t)=a` if `S_(t)=s`</p>
        <p>Value function of a state under a policy `pi`, denoted `v_(pi)(s)` is the expected return when starting in `s` and following `pi` thereafter.<br>
        For MDP,<br>
        &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`v_(pi)(s)&esdot; `&Eopf;<sub>`pi`</sub>`&#91;G_(t) | S_(t)=s&#93; =`&Eopf;<sub>`pi`</sub>`&#91;\sum_{k=0}^&infin; &gamma;^k R_(t+k+1) | S_(t)=s&#93;` &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; ..(1)</p>
        <p> This eq(1) is <i><b>state-value function for policy `pi`</i></b></p>
        <p>Similarly, value of taking action `a` in state `s` under a policy `pi`, denoted `q_(pi)(s,a)` as the expected return starting from `s`, can be written as:<br>
        &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`q_(pi)(s,a)=`&Eopf;<sub>`pi`</sub>`&#91;G_(t) | S_(t)=s, A_(t)=a&#93;=`&Eopf;<sub>`pi`</sub>`&#91;\sum_{k=0}^&infin; &gamma;^k R_(t+k+1) | S_(t)=s, A_(t)=a&#93;`&emsp;&emsp;&emsp;...(2)</p>
        <p>This eq(2) is <i><b>action-value function for policy `pi`</b></i></p>
        <p>1.if an agent follows policy `pi` and maintains an average of the actual returns that followed each state encountered; then the average will converge to state's value `v_(pi)(s)` as no.of times state is encountered approaches &infin; <br>
        2.If separate average are kept for each action taken in each state, then average will converge to <i>action-value `q_(pi)(s,a)`</i> These estimation method are called <i>Monte Carlo methods.</i></p>
        <p>For any policy `pi` and any state `s`, the following consistency condition hold between the value of `s` and the value of its possible successor states:</p>
        &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`v_(pi)(s) &esdot; `&Eopf;<sub>`pi`</sub>`&#91;G_(t) | S_(t)=s&#93;`<br>
        &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`=`&Eopf;<sub>`pi`</sub>`&#91;R_(t+1) + &gamma;G_(t+1) | S_(t)=s&#93;`<br>
        &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`=\sum_{a} pi(a|s)\sum_(s^')\sum_{r}p(s^',r |s,a)&#91;r +&gamma;`&Eopf;<sub>`pi`</sub>`&#91;G_(t+1) | S_(t+1)=s^'&#93;&#93;`<br>
        &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`v_(pi)(s)=\sum_{a} pi(a|s)\sum_(s^',r)p(s^',r |s,a)&#91;r +&gamma;v_(pi)(s^')&#93;`&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; ........(3)
        <p>This eq(3) is <i><b>Bellman Equation for `v_(pi)`</b></i>. It expresses a relationship between the value of state and the value of its successor states.</p>
        <p><b><u>Optimal Policies and Optimal Value Functions :</u></b></p>

        <p>A policy `pi` is defined to be better than or equal to policy `pi`' if its expected return is greater than or equal to that of `pi`' for all states i.e., `pi&ge; pi`' if and only if `v_(pi)(s)&ge;v_(pi)'`<br>
        Threrfore, <i> Optimal Policy</i> is a policy that is better than or equal to all other policies. However, it can be more than one. We denote all these optimal policies with `pi_(&star;)`, and they all share same state-value function called <i>optimal state-value function, `v_(&star;)`,</i>defined by:<br>
        &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`v_(&star;)(s) = \max_{pi}v_(pi)(s)`<br>
        <p>Optimal policies also share same <i>optimal action-value function</i>, `q_(&star;)`, defined by:<br>
          &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`q_(&star;)(s,a) &esdot;\max_{pi}q_(pi)(s,a)`</p>
        <p>For the state action pair `(s,a)` this function gives the expected return for taking action `a` in state `s` and thereafter following an optimal policy. Thus, we can write `q_(&star;)` in terms of `v_(&star;)` as follows:<br>
          &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`q_(&star;)(s,a) =` &Eopf;`&#91;R_(t+1) + &gamma;v_(&star;)(S_(t+1)) | S_(t)=s,A_(t)=a&#93;`</p>
        <p><b><u>Bellman Optimality Equation :</b></u></p>
        <p>It expresses the fact that the value of a state under an optimal policy must equal the expected return for the best action from the state. <i>Bellman Optimality Equation for `v_(&star;)`</i> therefore, is:</p>
        <p>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`v_(&star;)(s)=\max_{a&isin;A(s)}q_(pi&star;)(s,a)`<br>
          &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;`=\max_{a}`&Eopf;<sub>`pi_(&star;)`</sub>`&#91;G_(t) |S_(t)=s,A_(t)=a&#93;`<br>
          &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;`=\max_{a}`&Eopf;<sub>`pi_(&star;)`</sub>`&#91;R_(t+1) + &gamma;G_(t+1) | S_(t)=s, A_(t)=a&#93;`<br>
          &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`v_(&star;)(s)=\max_{a}`&Eopf;`&#91;R_(t+1) +&gamma;v_(&star;)(S_(t+1)) | S_(t)=s,A_(t)=a&#93;`<br>
          &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`v_(&star;)(s)=\max_{a}\sum_{s^'r}p(s',r |s,a)&#91;r +&gamma;v_(&star;)(s')&#93;`
        </p>
        <p>Similarly, <i>Bellman Optimality Equation for `q_(&star;)`</i>is: <br>
          &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`q_(&star;)(s,a)=`&Eopf;`&#91;R_(t+1) +&gamma;\max_{a'}q_(&star;)(s_(t+1),a')| S_(t)=s,A_(t)=a&#93;`<br>
          &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;`=\sum_{s^'r}p(s',r |s,a)&#91;r + &gamma; \max_{a'}q_(&star;)(s',a')&#93;`
        </p>

        </p>

</body>
</html>
